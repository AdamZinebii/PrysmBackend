import sys
sys.stdout.write("--- main.py PYTHON SCRIPT STARTED (STDOUT) ---\n")
sys.stderr.write("--- main.py PYTHON SCRIPT STARTED (STDERR) ---\n")
print("--- main.py PYTHON SCRIPT STARTED (PRINT) ---")

# Configure logging AS EARLY AS POSSIBLE
import logging
logging.basicConfig(stream=sys.stderr, level=logging.INFO, force=True, format='%(levelname)s:%(name)s:%(asctime)s:%(message)s')
logger = logging.getLogger(__name__)

# Welcome to Cloud Functions for Firebase for Python!
# Implementation of the Prysm backend for news aggregation

from firebase_functions import https_fn, scheduler_fn, options
from firebase_admin import initialize_app, firestore, messaging, storage, db
import openai
import feedparser
import urllib.parse
import json
import os
from datetime import datetime, timedelta
import serpapi
import re
from pathlib import Path
import uuid
import requests
import tempfile
import asyncio
from concurrent.futures import ThreadPoolExecutor
import time

logger.info("--- main.py: Logging configured ---")

# Initialize Firebase app
# Using Firestore (no need for database URL)
initialize_app()

# --- Constants ---
MAX_NEWS_SUBJECTS = 5
MAX_SPECIFIC_RESEARCH = 2
DETAIL_LEVELS = ["Light", "Medium", "Detailed"]
DETAIL_STARS = {
    "Light": "*",
    "Medium": "**", 
    "Detailed": "***"
}

SERPAPI_API_KEY_HARDCODED = "cc6fb3c2829269bca1fa87ecfeb3ff984e3313b5f2f80503ff1e55d8c6b9098c"

# GNews API Configuration
GNEWS_API_KEY = "75807d7923a12e3d80d64c971ff340da"  # GNews API key
GNEWS_BASE_URL = "https://gnews.io/api/v4"

# Function to get OpenAI API key
def get_openai_key():
    """Retrieve OpenAI API key from environment or fallback to hardcoded value."""
    key = os.environ.get("OPENAI_API_KEY")
    if key:
        return key
    logger.warning("OPENAI_API_KEY not found in env, using fallback key")
    return "sk-HxFKqwvTI8JfWp0WbPRF75FsoEQokPS2IQHrKIKQwtT3BlbkFJwNHRfzhJb-_Xz39M9531MNdy35DGxMkTZ2s05X2sYA"  # Replace with actual key

# Function to get OpenAI client
def get_openai_client():
    """Get configured OpenAI client."""
    try:
        api_key = get_openai_key()
        client = openai.OpenAI(api_key=api_key)
        logger.info("OpenAI client initialized successfully")
        return client
    except Exception as e:
        logger.error(f"Failed to initialize OpenAI client: {e}")
        return None

# Function to get GNews API key
def get_gnews_key():
    """Retrieve GNews API key from environment or fallback to hardcoded value."""
    key = os.environ.get("GNEWS_API_KEY")
    if key:
        return key
    logger.warning("GNEWS_API_KEY not found in env, using fallback key")
    return "03d7aa072298ddd7d1ad34170cd571ae"

def sanitize_gnews_query(query):
    """
    Sanitize query for GNews API by handling special characters.
    
    According to GNews documentation, special characters must be quoted.
    This function quotes queries that contain special characters to prevent syntax errors.
    
    Args:
        query (str): The original search query
        
    Returns:
        str: Sanitized query safe for GNews API
    """
    if not query:
        return query
    
    # Special characters that need quoting in GNews API (based on documentation)
    special_chars = ['&', '!', '?', '-', '+', '(', ')', '[', ']', '{', '}', ':', ';', '/', '\\', '<', '>', '=', '"']
    
    # Check if query contains any special characters
    has_special_chars = any(char in query for char in special_chars)
    
    # If query already has quotes, return as-is
    if query.startswith('"') and query.endswith('"'):
        return query
    
    # If query contains special characters, quote it
    if has_special_chars:
        # Escape any existing quotes inside the query
        escaped_query = query.replace('"', '\\"')
        return f'"{escaped_query}"'
    
    return query

# --- GNews API Functions ---

def gnews_search(query, lang="en", country="us", max_articles=10, from_date=None, to_date=None, nullable=None):
    """
    Search for news articles using GNews API search endpoint.
    
    Args:
        query (str): Search keywords
        lang (str): Language code (e.g., 'en', 'fr', 'es')
        country (str): Country code (e.g., 'us', 'fr', 'gb')
        max_articles (int): Number of articles to return (1-100)
        from_date (str): Filter articles from this date (YYYY-MM-DDThh:mm:ssZ)
        to_date (str): Filter articles to this date (YYYY-MM-DDThh:mm:ssZ)
        nullable (str): Comma-separated attributes that can be null (description,content,image)
    
    Returns:
        dict: API response with articles or error information
    """
    api_key = get_gnews_key()
    
    # Sanitize the query to handle special characters properly
    original_query = query
    sanitized_query = sanitize_gnews_query(query)
    logger.info(f"üîç GNews Search DEBUG: Original query: '{original_query}'")
    logger.info(f"üîç GNews Search DEBUG: Sanitized query: '{sanitized_query}'")
    
    # Try search with date filter first
    result = _perform_gnews_search(sanitized_query, lang, country, max_articles, from_date, to_date, nullable, api_key, original_query)
    
    # If no articles found with date filter, try without date filter as fallback
    if result.get('success') and result.get('totalArticles', 0) == 0 and from_date is not None:
        logger.info(f"üîÑ GNews Search DEBUG: No articles found with date filter for '{original_query}', trying without date filter as fallback")
        fallback_result = _perform_gnews_search(sanitized_query, lang, country, max_articles, None, to_date, nullable, api_key, original_query)
        
        if fallback_result.get('success') and fallback_result.get('totalArticles', 0) > 0:
            logger.info(f"‚úÖ GNews Search DEBUG: Fallback successful - found {fallback_result.get('totalArticles', 0)} articles without date filter")
            # Add a note that this used fallback
            fallback_result['used_fallback'] = True
            fallback_result['original_from_date'] = from_date
            return fallback_result
        else:
            logger.info(f"‚ö†Ô∏è GNews Search DEBUG: Fallback also returned no articles for '{original_query}'")
    
    return result

def _perform_gnews_search(sanitized_query, lang, country, max_articles, from_date, to_date, nullable, api_key, original_query):
    """Helper function to perform the actual GNews API search."""
    logger.info(f"üîç GNews Search DEBUG: Starting search for query '{original_query}'")
    
    url = "https://gnews.io/api/v4/search"
    
    # Build parameters
    params = {
        "q": sanitized_query,
        "lang": lang,
        "country": country,
        "max": min(max_articles, 100),  # API limit is 100
        "apikey": api_key
    }
    
    # Add optional parameters
    if from_date:
        params["from"] = from_date
    if to_date:
        params["to"] = to_date
    if nullable:
        params["nullable"] = nullable
    
    logger.info(f"üìã GNews Search DEBUG: Full URL: {url}")
    logger.info(f"üìã GNews Search DEBUG: Parameters: {params}")
    
    try:
        response = requests.get(url, params=params, timeout=30)
        
        logger.info(f"üì° GNews Search DEBUG: HTTP Status Code: {response.status_code}")
        logger.info(f"üì° GNews Search DEBUG: Response Headers: {dict(response.headers)}")
        
        if response.status_code == 200:
            data = response.json()
            logger.info(f"üìä GNews Search DEBUG: Raw response keys: {list(data.keys())}")
            
            total_articles = data.get('totalArticles', 0)
            articles = data.get('articles', [])
            
            logger.info(f"üìä GNews Search DEBUG: Total articles available: {total_articles}")
            logger.info(f"üìä GNews Search DEBUG: Articles returned: {len(articles)}")
            
            if articles:
                logger.info(f"üì∞ GNews Search DEBUG: First article title: {articles[0].get('title', 'No title')}")
                logger.info(f"üì∞ GNews Search DEBUG: First article source: {articles[0].get('source', {}).get('name', 'Unknown')}")
            else:
                logger.warning(f"‚ö†Ô∏è GNews Search DEBUG: No articles found for query '{original_query}' - this might be normal if the query is too specific or recent")
                logger.warning(f"‚ö†Ô∏è GNews Search DEBUG: Articles array is empty for query '{original_query}'")
            
            logger.info(f"‚úÖ GNews Search: Successfully fetched {len(articles)} articles out of {total_articles} total for query '{original_query}'")
            
            return {
                "success": True,
                "articles": articles,
                "totalArticles": total_articles,
                "query": original_query,
                "sanitized_query": sanitized_query
            }
            
        elif response.status_code == 429:
            logger.error(f"‚ùå GNews Search DEBUG: Too many requests (429) for query '{original_query}' - Rate limit exceeded")
            return {
                "success": False,
                "error": "Rate limit exceeded",
                "query": original_query
            }
        else:
            logger.error(f"‚ùå GNews Search DEBUG: Unexpected status code {response.status_code} for query '{original_query}'")
            try:
                error_data = response.json()
                logger.error(f"‚ùå GNews Search DEBUG: Response text: {response.text}")
                return {
                    "success": False,
                    "error": f"API error: {error_data}",
                    "status_code": response.status_code,
                    "query": original_query
                }
            except:
                return {
                    "success": False,
                    "error": f"HTTP {response.status_code}: {response.text}",
                    "status_code": response.status_code,
                    "query": original_query
                }
                
    except requests.RequestException as e:
        logger.error(f"‚ùå GNews Search: Request failed for query '{original_query}': {e}")
        return {
            "success": False,
            "error": f"Request failed: {str(e)}",
            "query": original_query
        }

def gnews_top_headlines(category="general", lang="en", country="us", max_articles=10, from_date=None, to_date=None, query=None, nullable=None):
    """
    Get top headlines using GNews API top-headlines endpoint.
    
    Args:
        category (str): News category (general, world, nation, business, technology, entertainment, sports, science, health)
        lang (str): Language code (e.g., 'en', 'fr', 'es')
        country (str): Country code (e.g., 'us', 'fr', 'gb')
        max_articles (int): Number of articles to return (1-100)
        from_date (str): Filter articles from this date (YYYY-MM-DDThh:mm:ssZ)
        to_date (str): Filter articles to this date (YYYY-MM-DDThh:mm:ssZ)
        query (str): Optional search keywords to narrow results
        nullable (str): Comma-separated attributes that can be null (description,content,image)
    
    Returns:
        dict: API response with articles or error information
    """
    api_key = get_gnews_key()
    if not api_key or api_key == "your-gnews-api-key-here":
        logger.error("GNews API key is not configured")
        return {"error": "GNews API key not configured", "articles": []}
    
    # Validate category
    valid_categories = ["general", "world", "nation", "business", "technology", "entertainment", "sports", "science", "health"]
    if category not in valid_categories:
        logger.warning(f"Invalid category '{category}', using 'general'")
        category = "general"
    
    # Build URL and parameters
    url = f"{GNEWS_BASE_URL}/top-headlines"
    params = {
        "category": category,
        "lang": lang,
        "country": country,
        "max": min(max_articles, 100),  # Ensure max is within API limits
        "apikey": api_key
    }
    
    # Add optional parameters
    if from_date:
        params["from"] = from_date
    if to_date:
        params["to"] = to_date
    if query:
        # Sanitize the query to handle special characters properly
        sanitized_query = sanitize_gnews_query(query)
        params["q"] = sanitized_query
    if nullable:
        params["nullable"] = nullable
    
    logger.info(f"GNews Top Headlines: Fetching {category} articles with params: {params}")
    
    try:
        response = requests.get(url, params=params, timeout=15)
        
        if response.status_code == 200:
            data = response.json()
            total_articles = data.get("totalArticles", 0)
            articles = data.get("articles", [])
            
            logger.info(f"GNews Top Headlines: Successfully fetched {len(articles)} articles out of {total_articles} total")
            return {
                "success": True,
                "totalArticles": total_articles,
                "articles": articles,
                "category": category,
                "params": params
            }
        elif response.status_code == 401:
            logger.error("GNews API: Unauthorized - Invalid API key")
            return {"error": "Invalid API key", "articles": []}
        elif response.status_code == 403:
            logger.error("GNews API: Forbidden - Daily quota reached")
            return {"error": "Daily quota reached", "articles": []}
        elif response.status_code == 429:
            logger.error("GNews API: Too many requests")
            return {"error": "Rate limit exceeded", "articles": []}
        else:
            logger.error(f"GNews API error {response.status_code}: {response.text}")
            return {"error": f"API error {response.status_code}", "articles": []}
            
    except requests.exceptions.Timeout:
        logger.error("GNews API request timed out")
        return {"error": "Request timeout", "articles": []}
    except requests.exceptions.RequestException as e:
        logger.error(f"GNews API request failed: {e}")
        return {"error": f"Request failed: {str(e)}", "articles": []}
    except Exception as e:
        logger.error(f"Unexpected error in GNews top headlines: {e}")
        return {"error": f"Unexpected error: {str(e)}", "articles": []}

def format_gnews_articles_for_prysm(gnews_response):
    """
    Convert GNews API response to Prysm-compatible format.
    
    Args:
        gnews_response (dict): Response from GNews API
    
    Returns:
        list: List of articles in Prysm format
    """
    if not gnews_response.get("success") or not gnews_response.get("articles"):
        return []
    
    formatted_articles = []
    
    for article in gnews_response["articles"]:
        formatted_article = {
            'title': article.get('title', '').strip(),
            'link': article.get('url', '#').strip(),
            'source': article.get('source', {}).get('name', 'Unknown Source'),
            'published': article.get('publishedAt', ''),
            'snippet': article.get('description', '').strip(),
            'thumbnail': article.get('image', ''),
            'content': article.get('content', '').strip()
        }
        
        # Clean empty fields
        formatted_article = {k: v for k, v in formatted_article.items() if v and v != 'No Title'}
        
        if formatted_article:  # Only add non-empty articles
            formatted_articles.append(formatted_article)
    
    logger.info(f"Formatted {len(formatted_articles)} articles for Prysm")
    return formatted_articles

# --- Basic Health Check Endpoint ---
@https_fn.on_request(timeout_sec=60)
def health_check(req: https_fn.Request) -> https_fn.Response:
    """Basic health check endpoint."""
    
    # Handle CORS preflight requests
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type, Authorization',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers, status=204)
        
    if req.method != 'GET':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response('Method not allowed. Use GET.', headers=headers, status=405)
    
    try:
        response_data = {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "message": "PrysmIOS Backend is running"
        }
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response(json.dumps(response_data), headers=headers)
        
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "status": "error",
            "message": str(e),
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)

# --- Placeholder for future endpoints ---
# TODO: Add your new endpoints here

# --- GNews API Test Endpoint ---
@https_fn.on_request(timeout_sec=120)
def test_gnews_api(req: https_fn.Request) -> https_fn.Response:
    """Test endpoint for GNews API functionality."""
    
    # Handle CORS preflight requests
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type, Authorization',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers, status=204)
        
    if req.method not in ['GET', 'POST']:
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response('Method not allowed. Use GET or POST.', headers=headers, status=405)
    
    try:
        # Get parameters from query string (GET) or JSON body (POST)
        if req.method == 'GET':
            endpoint = req.args.get('endpoint', 'search')
            query = req.args.get('query', 'technology')
            category = req.args.get('category', 'general')
            lang = req.args.get('lang', 'en')
            country = req.args.get('country', 'us')
            max_articles = int(req.args.get('max', '10'))
        else:  # POST
            data = req.get_json() or {}
            endpoint = data.get('endpoint', 'search')
            query = data.get('query', 'technology')
            category = data.get('category', 'general')
            lang = data.get('lang', 'en')
            country = data.get('country', 'us')
            max_articles = int(data.get('max', '10'))
        
        logger.info(f"Testing GNews API - Endpoint: {endpoint}, Query: {query}, Category: {category}")
        
        # Call appropriate GNews function
        if endpoint == 'search':
            gnews_response = gnews_search(
                query=query,
                lang=lang,
                country=country,
                max_articles=max_articles
            )
        elif endpoint == 'top-headlines':
            gnews_response = gnews_top_headlines(
                category=category,
                lang=lang,
                country=country,
                max_articles=max_articles,
                query=query if query != 'technology' else None  # Only add query if it's not the default
            )
        else:
            headers = {
                'Access-Control-Allow-Origin': '*',
                'Content-Type': 'application/json'
            }
            return https_fn.Response(
                json.dumps({"error": "Invalid endpoint. Use 'search' or 'top-headlines'"}),
                headers=headers,
                status=400
            )
        
        # Format articles for Prysm if successful
        formatted_articles = []
        if gnews_response.get("success"):
            formatted_articles = format_gnews_articles_for_prysm(gnews_response)
        
        # Prepare response
        response_data = {
            "endpoint": endpoint,
            "gnews_response": gnews_response,
            "formatted_articles": formatted_articles,
            "article_count": len(formatted_articles),
            "timestamp": datetime.now().isoformat()
        }
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response(json.dumps(response_data), headers=headers)
        
    except Exception as e:
        logger.error(f"Error in test_gnews_api: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "error": str(e),
            "message": "An error occurred while testing GNews API",
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)

@https_fn.on_request(timeout_sec=120)
def fetch_news_with_gnews(req: https_fn.Request) -> https_fn.Response:
    """Fetch news articles using GNews API for a specific topic/query."""
    
    # Handle CORS preflight requests
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type, Authorization',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers, status=204)
        
    if req.method not in ['GET', 'POST']:
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response('Method not allowed. Use GET or POST.', headers=headers, status=405)
    
    try:
        # Get parameters
        if req.method == 'GET':
            query = req.args.get('query')
            lang = req.args.get('lang', 'en')
            country = req.args.get('country', 'us')
            max_articles = int(req.args.get('max', '10'))
            use_headlines = req.args.get('use_headlines', 'false').lower() == 'true'
            category = req.args.get('category', 'general')
        else:  # POST
            data = req.get_json() or {}
            query = data.get('query')
            lang = data.get('lang', 'en')
            country = data.get('country', 'us')
            max_articles = int(data.get('max', '10'))
            use_headlines = data.get('use_headlines', False)
            category = data.get('category', 'general')
        
        if not query:
            headers = {
                'Access-Control-Allow-Origin': '*',
                'Content-Type': 'application/json'
            }
            return https_fn.Response(
                json.dumps({"error": "Missing 'query' parameter"}),
                headers=headers,
                status=400
            )
        
        logger.info(f"Fetching news for query: '{query}', lang: {lang}, country: {country}")
        
        # Choose endpoint based on use_headlines parameter
        if use_headlines:
            gnews_response = gnews_top_headlines(
                category=category,
                lang=lang,
                country=country,
                max_articles=max_articles,
                query=query
            )
        else:
            gnews_response = gnews_search(
                query=query,
                lang=lang,
                country=country,
                max_articles=max_articles
            )
        
        # Format articles for Prysm
        formatted_articles = format_gnews_articles_for_prysm(gnews_response)
        
        # Prepare response
        response_data = {
            "query": query,
            "success": gnews_response.get("success", False),
            "total_articles": gnews_response.get("totalArticles", 0),
            "returned_articles": len(formatted_articles),
            "articles": formatted_articles,
            "endpoint_used": "top-headlines" if use_headlines else "search",
            "error": gnews_response.get("error"),
            "timestamp": datetime.now().isoformat()
        }
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response(json.dumps(response_data), headers=headers)
        
    except Exception as e:
        logger.error(f"Error in fetch_news_with_gnews: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "error": str(e),
            "message": "An error occurred while fetching news",
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)

# --- Conversation System ---

def build_system_prompt(user_preferences):
    """
    Build the system prompt based on user preferences.
    
    Args:
        user_preferences (dict): User preferences including subjects, subtopics, detail_level, language, specific_subjects, etc.
    
    Returns:
        str: Complete system prompt for the AI
    """
    subjects = user_preferences.get('subjects', [])
    subtopics = user_preferences.get('subtopics', [])
    specific_subjects = user_preferences.get('specific_subjects', [])
    detail_level = user_preferences.get('detail_level', 'Medium')
    language = user_preferences.get('language', 'en')
    
    # Language-specific prompts
    language_prompts = {
        'en': {
            'role': "You are a preferences discovery assistant for PrysmIOS app.",
            'task': "Your ONLY goal is to discover the user's specific news interests and preferences. DO NOT provide news articles or current events. Keep responses SHORT (max 3-4 sentences).",
            'guide': "Ask questions to understand what specific topics, companies, people, or events they want to follow. Be proactive in discovering their interests.",
            'subjects_intro': "User selected:",
            'subtopics_intro': "Subtopics:",
            'detail_intro': f"Detail level: {detail_level.lower()}.",
            'refinement_task': "Ask about specific entities they want to follow from their topics. Examples: 'Which tech companies interest you?' or 'Any specific sports teams you follow?'",
            'guidelines': "DISCOVER PREFERENCES, DON'T GIVE NEWS! Examples: Technology ‚Üí Ask 'Which tech companies like Apple, Tesla, or OpenAI interest you?' Sports ‚Üí Ask 'Do you follow specific teams like Lakers or players like Messi?'",
            'conversation_flow': "When you have enough specific interests, say: 'Perfect! I've learned about your interests. Your personalized news feed is ready!'"
        },
        'fr': {
            'role': "Tu es un assistant de d√©couverte de pr√©f√©rences pour l'application PrysmIOS.",
            'task': "Ton SEUL objectif est de d√©couvrir les int√©r√™ts et pr√©f√©rences sp√©cifiques de l'utilisateur. NE DONNE PAS d'articles d'actualit√©s ou d'√©v√©nements actuels. Reste BREF (max 3-4 phrases).",
            'guide': "Pose des questions pour comprendre quels sujets sp√©cifiques, entreprises, personnes ou √©v√©nements ils veulent suivre. Sois proactif dans la d√©couverte de leurs int√©r√™ts.",
            'subjects_intro': "Utilisateur a choisi :",
            'subtopics_intro': "Sous-sujets :",
            'detail_intro': f"Niveau de d√©tail : {detail_level.lower()}.",
            'refinement_task': "Demande quelles entit√©s sp√©cifiques ils veulent suivre dans leurs sujets. Exemples : 'Quelles entreprises tech t'int√©ressent ?' ou 'Tu suis des √©quipes sportives particuli√®res ?'",
            'guidelines': "D√âCOUVRE LES PR√âF√âRENCES, NE DONNE PAS D'ACTUALIT√âS ! Exemples : Technologie ‚Üí Demande 'Quelles entreprises comme Apple, Tesla ou OpenAI t'int√©ressent ?' Sport ‚Üí Demande 'Tu suis des √©quipes comme le PSG ou des joueurs comme Mbapp√© ?'",
            'conversation_flow': "Quand tu as assez d'int√©r√™ts sp√©cifiques, dis : 'Parfait ! J'ai appris tes int√©r√™ts. Ton flux d'actualit√©s personnalis√© est pr√™t !'"
        },
        'es': {
            'role': "Eres un asistente de descubrimiento de preferencias para la aplicaci√≥n PrysmIOS.",
            'task': "Tu √öNICO objetivo es descubrir los intereses y preferencias espec√≠ficos del usuario. NO proporciones art√≠culos de noticias o eventos actuales. Mantente BREVE (m√°x 3-4 frases).",
            'guide': "Haz preguntas para entender qu√© temas espec√≠ficos, empresas, personas o eventos quieren seguir. S√© proactivo en descubrir sus intereses.",
            'subjects_intro': "Usuario eligi√≥:",
            'subtopics_intro': "Subtemas:",
            'detail_intro': f"Nivel de detalle: {detail_level.lower()}.",
            'refinement_task': "Pregunta qu√© entidades espec√≠ficas quieren seguir de sus temas. Ejemplos: '¬øQu√© empresas tecnol√≥gicas te interesan?' o '¬øSigues equipos deportivos espec√≠ficos?'",
            'guidelines': "¬°DESCUBRE PREFERENCIAS, NO DES NOTICIAS! Ejemplos: Tecnolog√≠a ‚Üí Pregunta '¬øQu√© empresas como Apple, Tesla u OpenAI te interesan?' Deportes ‚Üí Pregunta '¬øSigues equipos como Real Madrid o jugadores como Messi?'",
            'conversation_flow': "Cuando tengas suficientes intereses espec√≠ficos, di: '¬°Perfecto! He aprendido sobre tus intereses. ¬°Tu feed de noticias personalizado est√° listo!'"
        },
        'ar': {
            'role': "ÿ£ŸÜÿ™ ŸÖÿ≥ÿßÿπÿØ ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ™ŸÅÿ∂ŸäŸÑÿßÿ™ ŸÑÿ™ÿ∑ÿ®ŸäŸÇ PrysmIOS.",
            'task': "ŸáÿØŸÅŸÉ ÿßŸÑŸàÿ≠ŸäÿØ ŸáŸà ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸáÿ™ŸÖÿßŸÖÿßÿ™ Ÿàÿ™ŸÅÿ∂ŸäŸÑÿßÿ™ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÖÿ≠ÿØÿØÿ©. ŸÑÿß ÿ™ŸÇÿØŸÖ ŸÖŸÇÿßŸÑÿßÿ™ ÿ•ÿÆÿ®ÿßÿ±Ÿäÿ© ÿ£Ÿà ÿ£ÿ≠ÿØÿßÿ´ ÿ¨ÿßÿ±Ÿäÿ©. ŸÉŸÜ ŸÖÿÆÿ™ÿµÿ±ÿßŸã (ÿ≠ÿØ ÿ£ŸÇÿµŸâ 3-4 ÿ¨ŸÖŸÑ).",
            'guide': "ÿßÿ∑ÿ±ÿ≠ ÿ£ÿ≥ÿ¶ŸÑÿ© ŸÑŸÅŸáŸÖ ÿßŸÑŸÖŸàÿßÿ∂Ÿäÿπ ÿßŸÑŸÖÿ≠ÿØÿØÿ© ŸàÿßŸÑÿ¥ÿ±ŸÉÿßÿ™ ŸàÿßŸÑÿ£ÿ¥ÿÆÿßÿµ ÿ£Ÿà ÿßŸÑÿ£ÿ≠ÿØÿßÿ´ ÿßŸÑÿ™Ÿä Ÿäÿ±ŸäÿØŸàŸÜ ŸÖÿ™ÿßÿ®ÿπÿ™Ÿáÿß. ŸÉŸÜ ÿßÿ≥ÿ™ÿ®ÿßŸÇŸäÿßŸã ŸÅŸä ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸáÿ™ŸÖÿßŸÖÿßÿ™ŸáŸÖ.",
            'subjects_intro': "ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ÿßÿÆÿ™ÿßÿ±:",
            'subtopics_intro': "ÿßŸÑŸÖŸàÿßÿ∂Ÿäÿπ ÿßŸÑŸÅÿ±ÿπŸäÿ©:",
            'detail_intro': f"ŸÖÿ≥ÿ™ŸàŸâ ÿßŸÑÿ™ŸÅÿµŸäŸÑ: {detail_level.lower()}.",
            'refinement_task': "ÿßÿ≥ÿ£ŸÑ ÿπŸÜ ÿßŸÑŸÉŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ≠ÿØÿØÿ© ÿßŸÑÿ™Ÿä Ÿäÿ±ŸäÿØŸàŸÜ ŸÖÿ™ÿßÿ®ÿπÿ™Ÿáÿß ŸÖŸÜ ŸÖŸàÿßÿ∂ŸäÿπŸáŸÖ. ÿ£ŸÖÿ´ŸÑÿ©: 'ŸÖÿß ÿßŸÑÿ¥ÿ±ŸÉÿßÿ™ ÿßŸÑÿ™ŸÇŸÜŸäÿ© ÿßŸÑÿ™Ÿä ÿ™ŸáŸÖŸÉÿü' ÿ£Ÿà 'ŸáŸÑ ÿ™ÿ™ÿßÿ®ÿπ ŸÅÿ±ŸÇ ÿ±Ÿäÿßÿ∂Ÿäÿ© ŸÖÿπŸäŸÜÿ©ÿü'",
            'guidelines': "ÿßŸÉÿ™ÿ¥ŸÅ ÿßŸÑÿ™ŸÅÿ∂ŸäŸÑÿßÿ™ÿå ŸÑÿß ÿ™ÿπÿ∑Ÿê ÿ£ÿÆÿ®ÿßÿ±ÿßŸã! ÿ£ŸÖÿ´ŸÑÿ©: ÿßŸÑÿ™ŸÉŸÜŸàŸÑŸàÿ¨Ÿäÿß ‚Üí ÿßÿ≥ÿ£ŸÑ 'ŸÖÿß ÿßŸÑÿ¥ÿ±ŸÉÿßÿ™ ŸÖÿ´ŸÑ ÿ¢ÿ®ŸÑ ÿ£Ÿà ÿ™ÿ≥ŸÑÿß ÿ£Ÿà OpenAI ÿßŸÑÿ™Ÿä ÿ™ŸáŸÖŸÉÿü' ÿßŸÑÿ±Ÿäÿßÿ∂ÿ© ‚Üí ÿßÿ≥ÿ£ŸÑ 'ŸáŸÑ ÿ™ÿ™ÿßÿ®ÿπ ŸÅÿ±ŸÇ ŸÖÿ´ŸÑ ÿ±ŸäÿßŸÑ ŸÖÿØÿ±ŸäÿØ ÿ£Ÿà ŸÑÿßÿπÿ®ŸäŸÜ ŸÖÿ´ŸÑ ŸÖŸäÿ≥Ÿäÿü'",
            'conversation_flow': "ÿπŸÜÿØŸÖÿß ÿ™ÿ≠ÿµŸÑ ÿπŸÑŸâ ÿßŸáÿ™ŸÖÿßŸÖÿßÿ™ ŸÖÿ≠ÿØÿØÿ© ŸÉÿßŸÅŸäÿ©ÿå ŸÇŸÑ: 'ŸÖŸÖÿ™ÿßÿ≤! ŸÑŸÇÿØ ÿ™ÿπŸÑŸÖÿ™ ÿπŸÜ ÿßŸáÿ™ŸÖÿßŸÖÿßÿ™ŸÉ. ÿ™ÿØŸÅŸÇ ÿ£ÿÆÿ®ÿßÿ±ŸÉ ÿßŸÑÿ¥ÿÆÿµŸä ÿ¨ÿßŸáÿ≤!'"
        }
    }
    
    prompt_data = language_prompts.get(language, language_prompts['en'])
    
    # Build the complete system prompt
    system_prompt = f"""IMPORTANT: You are NOT a news provider. You do NOT give news articles, headlines, or current events.

{prompt_data['role']}

{prompt_data['task']}

{prompt_data['guide']}

{prompt_data['subjects_intro']} {', '.join(subjects) if subjects else 'None specified'}

"""
    
    # Add subtopics if available
    if subtopics:
        system_prompt += f"{prompt_data['subtopics_intro']} {', '.join(subtopics)}\n\n"
    
    system_prompt += f"""{prompt_data['detail_intro']}

{prompt_data['refinement_task']}

{prompt_data['guidelines']}

{prompt_data['conversation_flow']}"""
    
    return system_prompt

def format_conversation_history(messages):
    """
    Format conversation history for OpenAI API.
    
    Args:
        messages (list): List of message objects with 'role' and 'content'
    
    Returns:
        list: Formatted messages for OpenAI API
    """
    formatted_messages = []
    
    for message in messages:
        role = message.get('role', '').lower()
        content = message.get('content', '')
        
        # Map roles to OpenAI format
        if role in ['user', 'human']:
            formatted_messages.append({"role": "user", "content": content})
        elif role in ['assistant', 'chatbot', 'ai']:
            formatted_messages.append({"role": "assistant", "content": content})
        elif role == 'system':
            formatted_messages.append({"role": "system", "content": content})
    
    return formatted_messages

def generate_ai_response(system_prompt, conversation_history, user_message):
    """
    Generate AI response using OpenAI API.
    
    Args:
        system_prompt (str): System prompt with user preferences
        conversation_history (list): Previous messages in conversation
        user_message (str): Current user message
    
    Returns:
        dict: Response with success status and AI message or error
    """
    try:
        client = get_openai_client()
        if not client:
            return {"success": False, "error": "OpenAI client not available"}
        
        # Build messages array
        messages = [{"role": "system", "content": system_prompt}]
        
        # Add conversation history
        formatted_history = format_conversation_history(conversation_history)
        messages.extend(formatted_history)
        
        # Add current user message
        messages.append({"role": "user", "content": user_message})
        
        # Generate response - shorter limit for concise responses, use gpt-4o-mini model
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            max_tokens=150,
            temperature=0.7
        )
        
        ai_message = response.choices[0].message.content
        
        return {
            "success": True,
            "message": ai_message,
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
        }

    except Exception as e:
        logger.error(f"Error generating AI response: {e}")
        return {"success": False, "error": str(e)}

# --- Firebase Database Functions ---

def save_user_preferences_to_db(user_id, preferences_data):
    """
    Save user preferences to Firestore Database.
    
    Args:
        user_id (str): User ID
        preferences_data (dict): Preferences data with nested structure
                                New format v3.0:
                                {
                                    'preferences': {
                                        'business': {
                                            'Finance': {'subreddits': [...], 'queries': [...]}
                                        },
                                        'technology': {
                                            'AI': {'subreddits': [...], 'queries': [...]},
                                            'Gadgets': {'subreddits': [...], 'queries': [...]}
                                        }
                                    },
                                    'detail_level': 'Medium',
                                    'language': 'en',
                                    'format_version': '3.0'
                                }
    
    Returns:
        dict: Success status and any error
    """
    try:
        # Use Firestore instead of Realtime Database
        db_client = firestore.client()
        
        # Prepare data structure for new nested format
        format_version = preferences_data.get('format_version', '3.0')
        
        if format_version == '3.0':
            # New nested format
            data = {
                'preferences': preferences_data.get('preferences', {}),
                'detail_level': preferences_data.get('detail_level', 'Medium'),
                'language': preferences_data.get('language', 'en'),
                'format_version': '3.0',
                'updated_at': datetime.now().isoformat()
            }
            
            # Validate the new nested format
            if not isinstance(data['preferences'], dict):
                logger.error(f"Invalid preferences format for user {user_id}: expected dict, got {type(data['preferences'])}")
                return {"success": False, "error": "Invalid preferences format"}
            
            # Validate nested structure
            for topic_name, topic_subtopics in data['preferences'].items():
                if not isinstance(topic_subtopics, dict):
                    logger.error(f"Invalid topic structure for {topic_name}: expected dict, got {type(topic_subtopics)}")
                    return {"success": False, "error": f"Invalid topic structure for {topic_name}"}
                
                for subtopic_name, subtopic_data in topic_subtopics.items():
                    if not isinstance(subtopic_data, dict):
                        logger.error(f"Invalid subtopic data for {subtopic_name}: expected dict, got {type(subtopic_data)}")
                        return {"success": False, "error": f"Invalid subtopic data for {subtopic_name}"}
                    
                    if 'subreddits' not in subtopic_data or 'queries' not in subtopic_data:
                        logger.error(f"Missing required fields in subtopic {subtopic_name}")
                        return {"success": False, "error": f"Missing required fields in subtopic {subtopic_name}"}
                    
                    if not isinstance(subtopic_data['subreddits'], list) or not isinstance(subtopic_data['queries'], list):
                        logger.error(f"Invalid subreddits/queries format in subtopic {subtopic_name}")
                        return {"success": False, "error": f"Invalid subreddits/queries format in subtopic {subtopic_name}"}
            
            # Count topics and subtopics for logging
            topics_count = len(data['preferences'])
            subtopics_count = sum(len(topic_subtopics) for topic_subtopics in data['preferences'].values())
            
            logger.info(f"Preferences saved for user {user_id} in nested format v{data['format_version']}")
            logger.info(f"  - Topics: {topics_count} items")
            logger.info(f"  - Subtopics: {subtopics_count} items")
            
        else:
            # Legacy format (v2.0 or v1.0) - keep for backward compatibility
            data = {
                'topics': preferences_data.get('topics', []),
                'subtopics': preferences_data.get('subtopics', {}),
                'specific_subjects': preferences_data.get('specific_subjects', []),
                'detail_level': preferences_data.get('detail_level', 'Medium'),
                'language': preferences_data.get('language', 'en'),
                'format_version': preferences_data.get('format_version', '2.0'),
                'updated_at': datetime.now().isoformat()
            }
            
            logger.info(f"Preferences saved for user {user_id} in legacy format v{data['format_version']}")
        
        # Save to Firestore
        doc_ref = db_client.collection('preferences').document(user_id)
        doc_ref.set(data)
        
        return {"success": True}
        
    except Exception as e:
        logger.error(f"Error saving preferences to database: {e}")
        return {"success": False, "error": str(e)}

def update_specific_subjects_in_db(user_id, new_specific_subjects):
    """
    Update specific subjects in Firestore Database.
    
    Args:
        user_id (str): User ID
        new_specific_subjects (list): List of new specific subjects to add
    
    Returns:
        dict: Success status and any error
    """
    try:
        # Use Firestore instead of Realtime Database
        db_client = firestore.client()
        doc_ref = db_client.collection('preferences').document(user_id)
        
        # Get current preferences
        doc = doc_ref.get()
        current_data = doc.to_dict() if doc.exists else {}
        
        # Get existing specific subjects
        existing_subjects = current_data.get('specific_subjects', [])
        
        # Add new subjects (avoid duplicates)
        for subject in new_specific_subjects:
            if subject not in existing_subjects:
                existing_subjects.append(subject)
        
        # Update Firestore - use set with merge to handle non-existing documents
        doc_ref.set({
            'specific_subjects': existing_subjects,
            'updated_at': datetime.now().isoformat()
        }, merge=True)
        
        logger.info(f"Updated specific subjects for user {user_id}: {new_specific_subjects}")
        
        return {"success": True, "updated_subjects": existing_subjects}

    except Exception as e:
        logger.error(f"Error updating specific subjects: {e}")
        return {"success": False, "error": str(e)}

def get_user_preferences_from_db(user_id):
    """
    Get user preferences from Firestore Database.
    Handles v3.0 (nested), v2.0 (flat), and v1.0 (legacy) formats for backward compatibility.
    
    Args:
        user_id (str): User ID
    
    Returns:
        dict: User preferences or empty dict if not found
              New format v3.0:
              {
                  'preferences': {
                      'business': {
                          'Finance': {'subreddits': [...], 'queries': [...]}
                      },
                      'technology': {
                          'AI': {'subreddits': [...], 'queries': [...]},
                          'Gadgets': {'subreddits': [...], 'queries': [...]}
                      }
                  },
                  'detail_level': 'Medium',
                  'language': 'en',
                  'format_version': '3.0'
              }
    """
    try:
        # Use Firestore instead of Realtime Database
        db_client = firestore.client()
        doc_ref = db_client.collection('preferences').document(user_id)
        doc = doc_ref.get()
        
        if doc.exists:
            data = doc.to_dict()
            format_version = data.get('format_version', '1.0')
            
            logger.info(f"Retrieved preferences for user {user_id} in format v{format_version}")
            
            # Handle new nested format (v3.0)
            if format_version == '3.0':
                # Validate new nested format structure
                if 'preferences' in data and isinstance(data['preferences'], dict):
                    topics_count = len(data['preferences'])
                    subtopics_count = sum(len(topic_subtopics) for topic_subtopics in data['preferences'].values())
                    logger.info(f"  - Topics: {topics_count} items")
                    logger.info(f"  - Subtopics: {subtopics_count} items")
                    return data
                else:
                    logger.warning(f"Invalid v3.0 format structure for user {user_id}, converting from legacy")
                    format_version = '2.0'  # Fall back to conversion
            
            # Handle v2.0 format or convert from v1.0
            if format_version == '2.0' or 'topics' in data or 'subtopics' in data:
                logger.info(f"Converting v2.0/legacy preferences for user {user_id} to v3.0 nested format")
                
                # Get old format data
                old_topics = data.get('topics', [])
                old_subtopics = data.get('subtopics', {})
                
                # Convert old topics (if they were localized) to GNews format
                converted_topics = []
                if isinstance(old_topics, list):
                    for topic in old_topics:
                        gnews_topic = convert_old_topic_to_gnews(topic)
                        if gnews_topic not in converted_topics:
                            converted_topics.append(gnews_topic)
                
                # Convert old flat subtopics to new nested format
                nested_preferences = {}
                
                # Initialize topics in nested structure
                for topic in converted_topics:
                    nested_preferences[topic] = {}
                
                # Distribute subtopics under their parent topics
                if isinstance(old_subtopics, dict):
                    for subtopic_name, subtopic_data in old_subtopics.items():
                        # Find which topic this subtopic belongs to
                        parent_topic = find_parent_topic_for_subtopic(subtopic_name)
                        
                        if parent_topic and parent_topic in nested_preferences:
                            # Convert subtopic data format if needed
                            if isinstance(subtopic_data, dict) and 'subreddits' in subtopic_data and 'queries' in subtopic_data:
                                nested_preferences[parent_topic][subtopic_name] = subtopic_data
                            else:
                                # Create basic structure for legacy data
                                subtopic_meta = find_subtopic_in_catalog(subtopic_name)
                                nested_preferences[parent_topic][subtopic_name] = {
                                    'subreddits': subtopic_meta.get('subreddits', []) if subtopic_meta else [],
                                    'queries': [subtopic_meta.get('query', subtopic_name)] if subtopic_meta else [subtopic_name]
                                }
                        else:
                            # If we can't find a parent topic, put it under 'general'
                            if 'general' not in nested_preferences:
                                nested_preferences['general'] = {}
                            
                            subtopic_meta = find_subtopic_in_catalog(subtopic_name)
                            nested_preferences['general'][subtopic_name] = {
                                'subreddits': subtopic_meta.get('subreddits', []) if subtopic_meta else [],
                                'queries': [subtopic_meta.get('query', subtopic_name)] if subtopic_meta else [subtopic_name]
                            }
                
                # Create new v3.0 format structure
                converted_data = {
                    'preferences': nested_preferences,
                    'detail_level': data.get('detail_level', 'Medium'),
                    'language': data.get('language', 'en'),
                    'format_version': '3.0',
                    'updated_at': data.get('updated_at', datetime.now().isoformat()),
                    'converted_from': format_version
                }
                
                topics_count = len(nested_preferences)
                subtopics_count = sum(len(topic_subtopics) for topic_subtopics in nested_preferences.values())
                logger.info(f"Converted preferences for user {user_id}:")
                logger.info(f"  - Topics: {topics_count} items")
                logger.info(f"  - Subtopics: {subtopics_count} items")
                
                # Optionally save the converted format back to database
                try:
                    doc_ref.set(converted_data)
                    logger.info(f"Saved converted v3.0 preferences for user {user_id}")
                except Exception as e:
                    logger.warning(f"Failed to save converted preferences for user {user_id}: {e}")
                
                return converted_data
            
            # If we get here, it's an unknown format - return as is
            logger.warning(f"Unknown format version {format_version} for user {user_id}")
            return data
        else:
            logger.info(f"No preferences found for user {user_id}")
            return {}
            
    except Exception as e:
        logger.error(f"Error retrieving preferences: {e}")
        return {}

def find_parent_topic_for_subtopic(subtopic_name):
    """Find which topic a subtopic belongs to"""
    # Map subtopics to their parent topics
    subtopic_to_topic = {
        # Technology subtopics
        'AI': 'technology',
        'Artificial Intelligence': 'technology',
        'Gadgets': 'technology',
        'Software': 'technology',
        'Hardware': 'technology',
        'Cybersecurity': 'technology',
        'Startups': 'technology',
        
        # Business subtopics
        'Finance': 'business',
        'Economy': 'business',
        'Markets': 'business',
        'Cryptocurrency': 'business',
        'Investment': 'business',
        'Banking': 'business',
        
        # Sports subtopics
        'Football': 'sports',
        'Basketball': 'sports',
        'Tennis': 'sports',
        'Soccer': 'sports',
        'Olympics': 'sports',
        'Baseball': 'sports',
        
        # Science subtopics
        'Space': 'science',
        'Research': 'science',
        'Climate': 'science',
        'Physics': 'science',
        'Chemistry': 'science',
        'Biology': 'science',
        
        # Health subtopics
        'Medicine': 'health',
        'Fitness': 'health',
        'Nutrition': 'health',
        'Mental Health': 'health',
        'Wellness': 'health',
        
        # Entertainment subtopics
        'Movies': 'entertainment',
        'Music': 'entertainment',
        'Gaming': 'entertainment',
        'TV Shows': 'entertainment',
        'Celebrities': 'entertainment',
        
        # World subtopics
        'Politics': 'world',
        'International': 'world',
        'Conflicts': 'world',
        'Diplomacy': 'world'
    }
    
    return subtopic_to_topic.get(subtopic_name, 'general')

def convert_old_topic_to_gnews(old_topic):
    """Convert old topic format to GNews format"""
    if isinstance(old_topic, str):
        lowercased = old_topic.lower()
        
        # Map common old topics to GNews format
        topic_mapping = {
            'technology': 'technology',
            'technologie': 'technology',
            'tecnolog√≠a': 'technology',
            'ÿ™ŸÉŸÜŸàŸÑŸàÿ¨Ÿäÿß': 'technology',
            'business': 'business',
            'affaires': 'business',
            'negocios': 'business',
            'ÿ£ÿπŸÖÿßŸÑ': 'business',
            'sports': 'sports',
            'deportes': 'sports',
            'ÿ±Ÿäÿßÿ∂ÿ©': 'sports',
            'science': 'science',
            'ciencia': 'science',
            'ÿπŸÑŸàŸÖ': 'science',
            'health': 'health',
            'sant√©': 'health',
            'salud': 'health',
            'ÿµÿ≠ÿ©': 'health',
            'entertainment': 'entertainment',
            'divertissement': 'entertainment',
            'entretenimiento': 'entertainment',
            'ÿ™ÿ±ŸÅŸäŸá': 'entertainment',
            'world': 'world',
            'monde': 'world',
            'mundo': 'world',
            'ÿπÿßŸÑŸÖ': 'world',
            'general': 'general',
            'g√©n√©ral': 'general'
        }
        
        return topic_mapping.get(lowercased, 'general')
    
    return 'general'

def find_subtopic_in_catalog(subtopic_name):
    """Find subtopic metadata in our predefined catalog"""
    # This would need to be implemented based on your SubtopicsCatalog
    # For now, return a basic structure
    
    # Common subtopic mappings with basic subreddit suggestions
    subtopic_catalog = {
        'Artificial Intelligence': {
            'subreddits': ['MachineLearning', 'artificial', 'singularity'],
            'query': 'artificial intelligence OR AI'
        },
        'AI': {
            'subreddits': ['MachineLearning', 'artificial', 'singularity'],
            'query': 'artificial intelligence OR AI'
        },
        'Finance': {
            'subreddits': ['personalfinance', 'stocks', 'cryptocurrency'],
            'query': 'finance OR stock market OR investment'
        },
        'Gadgets': {
            'subreddits': ['gadgets', 'Android', 'apple'],
            'query': 'gadgets OR smartphones OR technology devices'
        },
        'Sports': {
            'subreddits': ['sports', 'nfl', 'nba'],
            'query': 'sports OR games OR athletics'
        }
    }
    
    return subtopic_catalog.get(subtopic_name, None)

# --- Specific Subjects Analysis ---

def analyze_conversation_for_specific_subjects(conversation_history, user_message, language='en'):
    """
    Analyze conversation to extract specific subjects using a separate LLM call.
    
    Args:
        conversation_history (list): Previous conversation messages
        user_message (str): Current user message
        language (str): Language code for analysis
    
    Returns:
        dict: Analysis result with extracted subjects
    """
    try:
        client = get_openai_client()
        if not client:
            return {"success": False, "error": "OpenAI client not available"}
        
        # Build analysis prompt based on language
        analysis_prompts = {
            'en': """CRITICAL TASK: Extract ONLY specific entities that the USER explicitly mentions in their messages.

RULES:
1. Look ONLY at messages that start with "user:"
2. Extract ONLY what the user explicitly names or mentions
3. IGNORE everything the assistant says
4. Extract specific names, companies, people, products, events, AND specific technologies

What to extract (ONLY if user mentions them):
- Company names: "Tesla", "Apple", "Microsoft", "OpenAI", "Google"
- People names: "Elon Musk", "Tim Cook", "Biden", "Cristiano Ronaldo"
- Products: "iPhone", "ChatGPT", "PlayStation", "Rubik's Cube"
- Events: "Olympics 2024", "CES", "World Cup"
- Specific technologies: "LLMs", "GPT-4", "machine learning", "AI", "robotique", "robot"
- Specific topics: "robot qui a battu le record", "innovations en robotique"

What NOT to extract:
- Very general concepts like "technology", "sports" (without specifics)
- Things only the assistant mentioned
- Implied topics not explicitly mentioned

IMPORTANT: If user says "LLMs", "robot", "robotique", "machine learning", "AI" - these ARE specific enough to extract.

Return ONLY a JSON array of specific entities the USER explicitly mentioned: ["entity1", "entity2"]
If user mentioned no specific entities, return: []""",
            
            'fr': """T√ÇCHE CRITIQUE: Extraire SEULEMENT les entit√©s sp√©cifiques que l'UTILISATEUR mentionne explicitement dans ses messages.

R√àGLES:
1. Regarde SEULEMENT les messages qui commencent par "user:"
2. Extrait SEULEMENT ce que l'utilisateur nomme ou mentionne explicitement
3. IGNORE tout ce que l'assistant dit
4. Extrait les noms sp√©cifiques, entreprises, personnes, produits, √©v√©nements, ET technologies sp√©cifiques

Quoi extraire (SEULEMENT si l'utilisateur les mentionne):
- Noms d'entreprises: "Tesla", "Apple", "Microsoft", "OpenAI", "Google"
- Noms de personnes: "Elon Musk", "Tim Cook", "Biden", "Cristiano Ronaldo"
- Produits: "iPhone", "ChatGPT", "PlayStation", "Rubik's Cube"
- √âv√©nements: "Jeux Olympiques 2024", "CES", "Coupe du Monde"
- Technologies sp√©cifiques: "LLMs", "GPT-4", "apprentissage automatique", "IA", "robotique", "robot"
- Sujets sp√©cifiques: "robot qui a battu le record", "innovations en robotique"

Quoi NE PAS extraire:
- Concepts tr√®s g√©n√©raux comme "technologie", "sport" (sans sp√©cificit√©s)
- Choses mentionn√©es seulement par l'assistant
- Sujets implicites non mentionn√©s explicitement

IMPORTANT: Si l'utilisateur dit "LLMs", "robot", "robotique", "apprentissage automatique", "IA" - ces termes SONT assez sp√©cifiques pour √™tre extraits.

Retourne SEULEMENT un array JSON d'entit√©s sp√©cifiques que l'UTILISATEUR a explicitement mentionn√©es: ["entit√©1", "entit√©2"]
Si l'utilisateur n'a mentionn√© aucune entit√© sp√©cifique, retourne: []""",
            
            'es': """TAREA CR√çTICA: Extraer SOLO entidades espec√≠ficas que el USUARIO menciona expl√≠citamente en sus mensajes.

REGLAS:
1. Mira SOLO mensajes que empiecen con "user:"
2. Extrae SOLO lo que el usuario nombra o menciona expl√≠citamente
3. IGNORA todo lo que dice el asistente
4. IGNORA temas generales como "IA", "tecnolog√≠a", "deportes"
5. Extrae SOLO nombres espec√≠ficos, empresas, personas, productos, eventos

Qu√© extraer (SOLO si el usuario los menciona):
- Nombres de empresas: "Tesla", "Apple", "Microsoft"
- Nombres de personas: "Elon Musk", "Tim Cook", "Biden"
- Productos: "iPhone", "ChatGPT", "PlayStation"
- Eventos: "Olimpiadas 2024", "CES", "Copa Mundial"
- Tecnolog√≠as espec√≠ficas: "LLMs" (si el usuario lo dice), "GPT-4"

Qu√© NO extraer:
- Conceptos generales: "IA", "tecnolog√≠a", "aprendizaje autom√°tico"
- Cosas mencionadas solo por el asistente
- Temas impl√≠citos o sugeridos

Devuelve SOLO un array JSON de entidades espec√≠ficas que el USUARIO mencion√≥ expl√≠citamente: ["entidad1", "entidad2"]
Si el usuario no mencion√≥ entidades espec√≠ficas, devuelve: []""",
            
            'ar': """ŸÖŸáŸÖÿ© ÿ≠ÿßÿ≥ŸÖÿ©: ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ŸÅŸÇÿ∑ ÿßŸÑŸÉŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ≠ÿØÿØÿ© ÿßŸÑÿ™Ÿä Ÿäÿ∞ŸÉÿ±Ÿáÿß ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ÿµÿ±ÿßÿ≠ÿ© ŸÅŸä ÿ±ÿ≥ÿßÿ¶ŸÑŸá.

ÿßŸÑŸÇŸàÿßÿπÿØ:
1. ÿßŸÜÿ∏ÿ± ŸÅŸÇÿ∑ ÿ•ŸÑŸâ ÿßŸÑÿ±ÿ≥ÿßÿ¶ŸÑ ÿßŸÑÿ™Ÿä ÿ™ÿ®ÿØÿ£ ÿ®ŸÄ "user:"
2. ÿßÿ≥ÿ™ÿÆÿ±ÿ¨ ŸÅŸÇÿ∑ ŸÖÿß Ÿäÿ≥ŸÖŸäŸá ÿ£Ÿà Ÿäÿ∞ŸÉÿ±Ÿá ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ÿµÿ±ÿßÿ≠ÿ©
3. ÿ™ÿ¨ÿßŸáŸÑ ŸÉŸÑ ŸÖÿß ŸäŸÇŸàŸÑŸá ÿßŸÑŸÖÿ≥ÿßÿπÿØ
4. ÿ™ÿ¨ÿßŸáŸÑ ÿßŸÑŸÖŸàÿßÿ∂Ÿäÿπ ÿßŸÑÿπÿßŸÖÿ© ŸÖÿ´ŸÑ "ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä"ÿå "ÿßŸÑÿ™ŸÉŸÜŸàŸÑŸàÿ¨Ÿäÿß"ÿå "ÿßŸÑÿ±Ÿäÿßÿ∂ÿ©"
5. ÿßÿ≥ÿ™ÿÆÿ±ÿ¨ ŸÅŸÇÿ∑ ÿßŸÑÿ£ÿ≥ŸÖÿßÿ° ÿßŸÑŸÖÿ≠ÿØÿØÿ©ÿå ÿßŸÑÿ¥ÿ±ŸÉÿßÿ™ÿå ÿßŸÑÿ£ÿ¥ÿÆÿßÿµÿå ÿßŸÑŸÖŸÜÿ™ÿ¨ÿßÿ™ÿå ÿßŸÑÿ£ÿ≠ÿØÿßÿ´

ŸÖÿß Ÿäÿ¨ÿ® ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨Ÿá (ŸÅŸÇÿ∑ ÿ•ÿ∞ÿß ÿ∞ŸÉÿ±Ÿáÿß ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ):
- ÿ£ÿ≥ŸÖÿßÿ° ÿßŸÑÿ¥ÿ±ŸÉÿßÿ™: "ÿ™ÿ≥ŸÑÿß"ÿå "ÿ¢ÿ®ŸÑ"ÿå "ŸÖÿßŸäŸÉÿ±Ÿàÿ≥ŸàŸÅÿ™"
- ÿ£ÿ≥ŸÖÿßÿ° ÿßŸÑÿ£ÿ¥ÿÆÿßÿµ: "ÿ•ŸäŸÑŸàŸÜ ŸÖÿßÿ≥ŸÉ"ÿå "ÿ™ŸäŸÖ ŸÉŸàŸÉ"ÿå "ÿ®ÿßŸäÿØŸÜ"
- ÿßŸÑŸÖŸÜÿ™ÿ¨ÿßÿ™: "ÿ¢ŸäŸÅŸàŸÜ"ÿå "ChatGPT"ÿå "ÿ®ŸÑÿßŸäÿ≥ÿ™Ÿäÿ¥ŸÜ"
- ÿßŸÑÿ£ÿ≠ÿØÿßÿ´: "ÿ£ŸàŸÑŸÖÿ®ŸäÿßÿØ 2024"ÿå "CES"ÿå "ŸÉÿ£ÿ≥ ÿßŸÑÿπÿßŸÑŸÖ"
- ÿßŸÑÿ™ŸÇŸÜŸäÿßÿ™ ÿßŸÑŸÖÿ≠ÿØÿØÿ©: "LLMs" (ÿ•ÿ∞ÿß ŸÇÿßŸÑŸáÿß ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ)ÿå "GPT-4"

ŸÖÿß ŸÑÿß Ÿäÿ¨ÿ® ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨Ÿá:
- ÿßŸÑŸÖŸÅÿßŸáŸäŸÖ ÿßŸÑÿπÿßŸÖÿ©: "ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä"ÿå "ÿßŸÑÿ™ŸÉŸÜŸàŸÑŸàÿ¨Ÿäÿß"ÿå "ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ¢ŸÑŸä"
- ÿßŸÑÿ£ÿ¥Ÿäÿßÿ° ÿßŸÑÿ™Ÿä ÿ∞ŸÉÿ±Ÿáÿß ÿßŸÑŸÖÿ≥ÿßÿπÿØ ŸÅŸÇÿ∑
- ÿßŸÑŸÖŸàÿßÿ∂Ÿäÿπ ÿßŸÑÿ∂ŸÖŸÜŸäÿ© ÿ£Ÿà ÿßŸÑŸÖŸÇÿ™ÿ±ÿ≠ÿ©

ÿ£ÿ±ÿ¨ÿπ ŸÅŸÇÿ∑ ŸÖÿµŸÅŸàŸÅÿ© JSON ŸÑŸÑŸÉŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ≠ÿØÿØÿ© ÿßŸÑÿ™Ÿä ÿ∞ŸÉÿ±Ÿáÿß ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ÿµÿ±ÿßÿ≠ÿ©: ["ŸÉŸäÿßŸÜ1", "ŸÉŸäÿßŸÜ2"]
ÿ•ÿ∞ÿß ŸÑŸÖ Ÿäÿ∞ŸÉÿ± ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ÿ£Ÿä ŸÉŸäÿßŸÜÿßÿ™ ŸÖÿ≠ÿØÿØÿ©ÿå ÿ£ÿ±ÿ¨ÿπ: []"""
        }
        
        analysis_prompt = analysis_prompts.get(language, analysis_prompts['en'])
        
        # Build conversation context
        conversation_text = ""
        for msg in conversation_history[-5:]:  # Last 5 messages for context
            role = msg.get('role', '')
            content = msg.get('content', '')
            conversation_text += f"{role}: {content}\n"
        
        conversation_text += f"user: {user_message}\n"
        
        # Create analysis messages
        messages = [
            {"role": "system", "content": analysis_prompt},
            {"role": "user", "content": f"Conversation to analyze:\n{conversation_text}"}
        ]
        
        # Generate analysis
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            max_tokens=200,
            temperature=0.3
        )
        
        analysis_result = response.choices[0].message.content.strip()
        
        # Try to parse JSON
        try:
            specific_subjects = json.loads(analysis_result)
            if isinstance(specific_subjects, list):
                # Filter out empty strings and duplicates
                specific_subjects = list(set([s.strip() for s in specific_subjects if s.strip()]))
                
                return {
                    "success": True,
                    "specific_subjects": specific_subjects,
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    }
                }
            else:
                return {"success": False, "error": "Invalid response format"}
                
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse analysis result as JSON: {analysis_result}")
            return {"success": False, "error": "Failed to parse analysis result"}
            
    except Exception as e:
        logger.error(f"Error analyzing conversation for specific subjects: {e}")
        return {"success": False, "error": str(e)}

# --- Background Analysis Helper ---

def analyze_and_update_specific_subjects(user_id, conversation_history, user_message, language):
    """
    Background function to analyze conversation and update specific subjects.
    This runs in a separate thread to not block the main conversation response.
    """
    try:
        logger.info(f"Background analysis started for user {user_id}")
        
        # Analyze conversation for specific subjects
        analysis_result = analyze_conversation_for_specific_subjects(
            conversation_history, user_message, language
        )
        
        if analysis_result["success"] and analysis_result.get("specific_subjects"):
            # Update database with new specific subjects
            update_result = update_specific_subjects_in_db(
                user_id, analysis_result["specific_subjects"]
            )
            
            if update_result["success"]:
                logger.info(f"Background analysis completed for user {user_id}. New subjects: {analysis_result['specific_subjects']}")
        else:
            logger.info(f"Background analysis completed for user {user_id}. No new subjects found.")
            
    except Exception as e:
        logger.error(f"Error in background analysis for user {user_id}: {e}")

# --- New Firebase Functions ---

@https_fn.on_request(timeout_sec=60)
def save_initial_preferences(req: https_fn.Request) -> https_fn.Response:
    """
    Save initial user preferences to Firestore Database.
    
    Expected JSON payload (NEW NESTED FORMAT):
    {
        "user_id": "user123",
        "preferences": {
            "business": {
                "Finance": {
                    "subreddits": ["personalfinance", "stocks", "cryptocurrency"],
                    "queries": ["stock market", "bitcoin", "interest rates"]
                }
            },
            "technology": {
                "AI": {
                    "subreddits": ["MachineLearning", "ArtificialInteligence", "singularity"],
                    "queries": ["openai", "chatgpt", "large language models"]
                },
                "Gadgets": {
                    "subreddits": ["gadgets", "Android", "apple"],
                    "queries": ["smartphones", "wearables", "iPhone 15"]
                }
            }
        },
        "detail_level": "Medium",
        "language": "en"
    }
    """
    
    # Handle CORS preflight requests
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type, Authorization',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers, status=204)
    
    if req.method != 'POST':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response('Method not allowed. Use POST.', headers=headers, status=405)
    
    try:
        # Parse request data
        data = req.get_json() or {}
        
        user_id = data.get('user_id')
        preferences = data.get('preferences', {})  # New nested structure
        detail_level = data.get('detail_level', 'Medium')
        language = data.get('language', 'en')
        
        # Validate required fields
        if not user_id:
            headers = {
                'Access-Control-Allow-Origin': '*',
                'Content-Type': 'application/json'
            }
            return https_fn.Response(
                json.dumps({"error": "Missing 'user_id' field"}),
                headers=headers,
                status=400
            )
        
        # Validate preferences format (nested structure)
        if not isinstance(preferences, dict):
            headers = {
                'Access-Control-Allow-Origin': '*',
                'Content-Type': 'application/json'
            }
            return https_fn.Response(
                json.dumps({"error": "'preferences' must be an object with nested topic structure"}),
                headers=headers,
                status=400
            )
        
        # Validate nested structure: topics -> subtopics -> {subreddits, queries}
        for topic_name, topic_subtopics in preferences.items():
            if not isinstance(topic_subtopics, dict):
                headers = {
                    'Access-Control-Allow-Origin': '*',
                    'Content-Type': 'application/json'
                }
                return https_fn.Response(
                    json.dumps({"error": f"Topic '{topic_name}' must contain subtopics as an object"}),
                    headers=headers,
                    status=400
                )
            
            for subtopic_name, subtopic_data in topic_subtopics.items():
                if not isinstance(subtopic_data, dict):
                    headers = {
                        'Access-Control-Allow-Origin': '*',
                        'Content-Type': 'application/json'
                    }
                    return https_fn.Response(
                        json.dumps({"error": f"Subtopic '{subtopic_name}' in topic '{topic_name}' must have an object with 'subreddits' and 'queries'"}),
                        headers=headers,
                        status=400
                    )
                
                if 'subreddits' not in subtopic_data or 'queries' not in subtopic_data:
                    headers = {
                        'Access-Control-Allow-Origin': '*',
                        'Content-Type': 'application/json'
                    }
                    return https_fn.Response(
                        json.dumps({"error": f"Subtopic '{subtopic_name}' must have 'subreddits' and 'queries' fields"}),
                        headers=headers,
                        status=400
                    )
                
                if not isinstance(subtopic_data['subreddits'], list) or not isinstance(subtopic_data['queries'], list):
                    headers = {
                        'Access-Control-Allow-Origin': '*',
                        'Content-Type': 'application/json'
                    }
                    return https_fn.Response(
                        json.dumps({"error": f"Subtopic '{subtopic_name}' subreddits and queries must be arrays"}),
                        headers=headers,
                        status=400
                    )
        
        # Prepare preferences data in new nested format
        preferences_data = {
            'preferences': preferences,  # New nested structure
            'detail_level': detail_level,
            'language': language,
            'format_version': '3.0'  # Version marker for the new nested format
        }
        
        # Count topics and subtopics for logging
        topics_count = len(preferences)
        subtopics_count = sum(len(topic_subtopics) for topic_subtopics in preferences.values())
        
        logger.info(f"Saving preferences for user {user_id} in new nested format v3.0")
        logger.info(f"Topics: {list(preferences.keys())}")
        logger.info(f"Topics count: {topics_count}")
        logger.info(f"Subtopics count: {subtopics_count}")
        
        # Save to database
        result = save_user_preferences_to_db(user_id, preferences_data)
        
        if result["success"]:
            response_data = {
                "success": True,
                "message": "Initial preferences saved successfully in new nested format",
                "user_id": user_id,
                "format_version": "3.0",
                "topics_count": topics_count,
                "subtopics_count": subtopics_count,
                "timestamp": datetime.now().isoformat()
            }
        else:
            response_data = {
                "success": False,
                "error": result.get("error", "Failed to save preferences"),
                "timestamp": datetime.now().isoformat()
            }
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        
        return https_fn.Response(json.dumps(response_data), headers=headers)
        
    except Exception as e:
        logger.error(f"Error in save_initial_preferences: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "success": False,
            "error": str(e),
            "message": "An error occurred while saving preferences",
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)

@https_fn.on_request(timeout_sec=60)
def update_specific_subjects(req: https_fn.Request) -> https_fn.Response:
    """
    Update specific subjects for a user based on conversation analysis.
    This function is called in parallel after each user message.
    
    Expected JSON payload:
    {
        "user_id": "user123",
        "conversation_history": [...],
        "user_message": "I'm interested in Tesla and SpaceX",
        "language": "en"
    }
    """
    
    # Handle CORS preflight requests
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type, Authorization',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers, status=204)
    
    if req.method != 'POST':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response('Method not allowed. Use POST.', headers=headers, status=405)
    
    try:
        # Parse request data
        data = req.get_json() or {}
        
        user_id = data.get('user_id')
        action = data.get('action', 'analyze')  # 'analyze' or 'get'
        conversation_history = data.get('conversation_history', [])
        user_message = data.get('user_message', '')
        language = data.get('language', 'en')
        
        # Validate required fields
        if not user_id:
            headers = {
                'Access-Control-Allow-Origin': '*',
                'Content-Type': 'application/json'
            }
            return https_fn.Response(
                json.dumps({"error": "Missing 'user_id' field"}),
                headers=headers,
                status=400
            )
        
        # Handle 'get' action - just return existing specific subjects
        if action == 'get':
            try:
                existing_preferences = get_user_preferences_from_db(user_id)
                specific_subjects = existing_preferences.get('specific_subjects', []) if existing_preferences else []
                response_data = {
                    "success": True,
                    "specific_subjects": specific_subjects,
                    "total_subjects": len(specific_subjects),
                    "timestamp": datetime.now().isoformat()
                }
                headers = {
                    'Access-Control-Allow-Origin': '*',
                    'Content-Type': 'application/json'
                }
                return https_fn.Response(json.dumps(response_data), headers=headers)
            except Exception as e:
                logger.error(f"Error getting specific subjects: {e}")
                response_data = {
                    "success": True,
                    "specific_subjects": [],
                    "total_subjects": 0,
                    "timestamp": datetime.now().isoformat()
                }
                headers = {
                    'Access-Control-Allow-Origin': '*',
                    'Content-Type': 'application/json'
                }
                return https_fn.Response(json.dumps(response_data), headers=headers)
        
        # For 'analyze' action, we need user_message
        if not user_message:
            headers = {
                'Access-Control-Allow-Origin': '*',
                'Content-Type': 'application/json'
            }
            return https_fn.Response(
                json.dumps({"error": "Missing 'user_message' field for analyze action"}),
                headers=headers,
                status=400
            )
        
        logger.info(f"Analyzing conversation for user {user_id}")
        
        # Analyze conversation for specific subjects
        analysis_result = analyze_conversation_for_specific_subjects(
            conversation_history, user_message, language
        )
        
        if analysis_result["success"] and analysis_result.get("specific_subjects"):
            # Update database with new specific subjects
            update_result = update_specific_subjects_in_db(
                user_id, analysis_result["specific_subjects"]
            )
            
            response_data = {
                "success": True,
                "new_subjects_found": analysis_result["specific_subjects"],
                "total_subjects": update_result.get("updated_subjects", []),
                "analysis_usage": analysis_result.get("usage", {}),
                "timestamp": datetime.now().isoformat()
            }
        else:
            response_data = {
                "success": True,
                "new_subjects_found": [],
                "message": "No new specific subjects found in this message",
                "timestamp": datetime.now().isoformat()
            }
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        
        return https_fn.Response(json.dumps(response_data), headers=headers)
        
    except Exception as e:
        logger.error(f"Error in update_specific_subjects: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "success": False,
            "error": str(e),
            "message": "An error occurred while updating specific subjects",
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)

@https_fn.on_request(timeout_sec=120)
def answer(req: https_fn.Request) -> https_fn.Response:
    """
    Handle conversation with AI assistant based on user preferences.
    
    Expected JSON payload:
    {
        "user_id": "user123",  # Optional - for saving specific subjects
        "user_preferences": {
            "subjects": ["technology", "sports"],
            "subtopics": ["AI", "Tennis"],
            "detail_level": "Medium",
            "language": "en"
        },
        "conversation_history": [
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": "Hi! How can I help you?"}
        ],
        "user_message": "I want to know about tech news"
    }
    """
    
    # Handle CORS preflight requests
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type, Authorization',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers, status=204)
    
    if req.method != 'POST':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response('Method not allowed. Use POST.', headers=headers, status=405)
    
    try:
        # Parse request data
        data = req.get_json() or {}
        
        user_id = data.get('user_id')  # Optional for specific subjects tracking
        user_preferences = data.get('user_preferences', {})
        conversation_history = data.get('conversation_history', [])
        user_message = data.get('user_message', '')
        
        # Validate required fields
        if not user_message:
            headers = {
                'Access-Control-Allow-Origin': '*',
                'Content-Type': 'application/json'
            }
            return https_fn.Response(
                json.dumps({"error": "Missing 'user_message' field"}),
                headers=headers,
                status=400
            )
        
        logger.info(f"Processing conversation - User ID: {user_id}")
        logger.info(f"User message: {user_message}")
        logger.info(f"Using current local preferences: {user_preferences}")
        
        # Always use the preferences sent in the request (current local preferences)
        # These are the user's current choices, not what's saved in database
        
        # Build system prompt based on current user preferences
        system_prompt = build_system_prompt(user_preferences)
        
        # Check if user wants to end conversation
        end_conversation_keywords = {
            'en': ['yes', 'sure', 'ok', 'okay', 'start reading', 'read news', 'go ahead', 'let\'s go'],
            'fr': ['oui', 'bien s√ªr', 'd\'accord', 'ok', 'commencer', 'lire', 'allons-y', 'c\'est parti'],
            'es': ['s√≠', 'claro', 'de acuerdo', 'ok', 'empezar', 'leer', 'vamos', 'adelante'],
            'ar': ['ŸÜÿπŸÖ', 'ŸÖŸàÿßŸÅŸÇ', 'ÿ≠ÿ≥ŸÜÿßŸã', 'ÿßÿ®ÿØÿ£', 'ÿßŸÇÿ±ÿ£', 'ŸáŸäÿß']
        }
        
        user_language = user_preferences.get('language', 'en')
        user_msg_lower = user_message.lower().strip()
        
        # Check if this might be a conversation ending response
        is_ending_response = False
        if user_language in end_conversation_keywords:
            keywords = end_conversation_keywords[user_language]
            is_ending_response = any(keyword in user_msg_lower for keyword in keywords)
        
        # Generate AI response
        ai_response = generate_ai_response(system_prompt, conversation_history, user_message)
        
        # If user_id is provided, analyze for specific subjects synchronously
        # This ensures the analysis happens but may add slight delay
        if user_id and user_message.strip():
            try:
                # Run analysis synchronously to ensure it completes
                analyze_and_update_specific_subjects(
                    user_id,
                    conversation_history,
                    user_message,
                    user_preferences.get('language', 'en')
                )
                logger.info(f"Completed analysis for user {user_id}")
            except Exception as e:
                logger.warning(f"Failed to analyze specific subjects: {e}")
                # Don't fail the main response if analysis fails
        
        if not ai_response["success"]:
            headers = {
                'Access-Control-Allow-Origin': '*',
                'Content-Type': 'application/json'
            }
            return https_fn.Response(
                json.dumps({
                    "error": "Failed to generate AI response",
                    "details": ai_response.get("error")
                }),
                headers=headers,
                status=500
            )
        
        # Check if AI suggests ending the conversation
        ai_message = ai_response["message"].lower()
        ai_suggests_ending = any(phrase in ai_message for phrase in [
            'personalized news feed is ready', 'flux d\'actualit√©s personnalis√© est pr√™t', 
            'feed de noticias personalizado est√° listo', 'ÿ™ÿØŸÅŸÇ ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸÖÿÆÿµÿµ ŸÑŸÉ ÿ¨ÿßŸáÿ≤',
            'start reading', 'commencer √† lire', 'empezar a leer', 'ÿßŸÑÿ®ÿØÿ° ŸÅŸä ŸÇÿ±ÿßÿ°ÿ©'
        ])
        
        # Prepare response
        response_data = {
            "success": True,
            "ai_message": ai_response["message"],
            "conversation_id": str(uuid.uuid4()),  # Generate conversation ID for tracking
            "timestamp": datetime.now().isoformat(),
            "usage": ai_response.get("usage", {}),
            "user_preferences": user_preferences,
            "conversation_ending": is_ending_response or ai_suggests_ending,
            "ready_for_news": ai_suggests_ending
        }
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        
        logger.info(f"AI response generated successfully: {len(ai_response['message'])} characters")
        
        return https_fn.Response(json.dumps(response_data), headers=headers)
        
    except Exception as e:
        logger.error(f"Error in answer function: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "success": False,
            "error": str(e),
            "message": "An error occurred while processing the conversation",
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)

# --- Trending Subtopics Analysis ---

def get_trending_topics_for_subtopic(subtopic_title, subtopic_query, subreddits, lang="en", country="us", max_articles=10):
    """
    Get trending topics for a specific subtopic by combining GNews search + Reddit posts.
    
    Args:
        subtopic_title (str): Display name (e.g., "Artificial Intelligence")
        subtopic_query (str): Search query (e.g., "artificial intelligence OR AI")
        subreddits (list): Associated subreddits (e.g., ["MachineLearning", "Artificial", "singularity"])
        lang (str): Language code
        country (str): Country code
        max_articles (int): Max GNews articles to fetch
    
    Returns:
        dict: Response with trending topics list
    """
    try:
        logger.info(f"Getting trending topics for subtopic: {subtopic_title}")
        
        # Step 1: Fetch GNews articles using subtopic query
        gnews_response = gnews_search(
            query=subtopic_query,
            lang=lang,
            country=country,
            max_articles=max_articles
        )
        
        gnews_articles = gnews_response.get("articles", []) if gnews_response.get("success") else []
        
        # Step 2: Fetch Reddit posts from associated subreddits
        reddit_posts = []
        headers = {"User-Agent": "NewsXTrendingBot/1.0"}
        
        for subreddit in subreddits[:3]:  # Limit to top 3 subreddits
            try:
                url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit=5"
                response = requests.get(url, headers=headers, timeout=10)
                response.raise_for_status()
                data = response.json()
                
                if "data" in data and "children" in data["data"]:
                    posts = data["data"]["children"]
                    for post in posts:
                        post_data = post.get("data", {})
                        reddit_posts.append({
                            "title": post_data.get("title", ""),
                            "score": post_data.get("score", 0),
                            "subreddit": subreddit
                        })
            except Exception as e:
                logger.warning(f"Failed to fetch from r/{subreddit}: {e}")
        
        # Step 3: Prepare content for LLM analysis
        content_text = f"SUBTOPIC: {subtopic_title}\n\n"
        
        # Add GNews articles
        content_text += "NEWS ARTICLES:\n"
        for i, article in enumerate(gnews_articles[:8], 1):
            title = article.get('title', '')
            description = article.get('description', '')
            content_text += f"{i}. {title}\n"
            if description:
                content_text += f"   {description}\n"
        
        # Add Reddit posts
        content_text += "\nREDDIT DISCUSSIONS:\n"
        for i, post in enumerate(reddit_posts[:10], 1):
            content_text += f"{i}. r/{post['subreddit']}: {post['title']} ({post['score']} points)\n"
        
        # Step 4: LLM prompt for trending topics extraction
        analysis_prompt = f"""You are a trending topics analyst. Based on the news articles and Reddit discussions below about "{subtopic_title}", extract 6-8 specific trending topics that are currently hot.

RULES:
1. Focus on SPECIFIC trending themes, events, companies, or technologies
2. Each topic should be 2-4 words maximum
3. Avoid generic terms - be specific about what's trending NOW
4. Look for patterns across both news and Reddit discussions
5. Prioritize topics mentioned in multiple sources
6. Return ONLY the trending topics, separated by commas
7. Topics should be suitable for news searches

CONTENT TO ANALYZE:
{content_text}

OUTPUT FORMAT: Return only the trending topics separated by commas, nothing else.
Example: "ChatGPT-4o launch, AI regulation EU, OpenAI funding round, LLM hallucination fix"
"""

        # Step 5: Get LLM analysis
        client = get_openai_client()
        if not client:
            return {
                "success": False,
                "error": "OpenAI client not available",
                "trending_topics": []
            }
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a trending topics analyst that extracts specific trending themes from news and social media content."},
                {"role": "user", "content": analysis_prompt}
            ],
            max_tokens=200,
            temperature=0.3
        )
        
        # Step 6: Parse LLM response
        llm_response = response.choices[0].message.content.strip()
        logger.info(f"LLM response: {llm_response}")
        
        # Extract trending topics
        trending_topics = []
        if llm_response:
            raw_topics = [s.strip() for s in llm_response.split(',')]
            trending_topics = [t for t in raw_topics if t and len(t) > 2]
        
        logger.info(f"Extracted {len(trending_topics)} trending topics: {trending_topics}")
        
        return {
            "success": True,
            "subtopic": subtopic_title,
            "gnews_articles_count": len(gnews_articles),
            "reddit_posts_count": len(reddit_posts),
            "trending_topics": trending_topics,
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting trending topics for subtopic: {e}")
        return {
            "success": False,
            "error": str(e),
            "trending_topics": []
        }

def extract_trending_subtopics(topic, lang="en", country="us", max_articles=10):
    """
    Extract trending subtopics from news articles for a given topic using LLM analysis.
    
    Args:
        topic (str): The main topic/category to analyze (e.g., "technology", "sports", "business")
        lang (str): Language code (e.g., 'en', 'fr', 'es')
        country (str): Country code (e.g., 'us', 'fr', 'gb')
        max_articles (int): Number of articles to analyze (default 10)
    
    Returns:
        dict: Response with success status and list of trending subtopic keywords
    """
    try:
        logger.info(f"Extracting trending subtopics for topic: {topic}")
        
        # Step 1: Fetch headlines using existing function
        gnews_response = gnews_top_headlines(
            category=topic.lower(),
            lang=lang,
            country=country,
            max_articles=max_articles
        )
        
        # Check if we got articles
        if not gnews_response.get("success") or not gnews_response.get("articles"):
            logger.warning(f"No articles found for topic: {topic}")
            return {
                "success": False,
                "error": f"No articles found for topic: {topic}",
                "subtopics": []
            }
        
        articles = gnews_response["articles"]
        logger.info(f"Fetched {len(articles)} articles for analysis")
        
        # Step 2: Prepare articles text for LLM analysis
        articles_text = ""
        for i, article in enumerate(articles[:10], 1):  # Limit to first 10 articles
            title = article.get('title', '')
            description = article.get('description', '')
            
            articles_text += f"Article {i}:\n"
            articles_text += f"Title: {title}\n"
            if description:
                articles_text += f"Description: {description}\n"
            articles_text += "\n"
        
        # Step 3: Create LLM prompt for subtopic extraction
        analysis_prompt = f"""You are a news analysis expert. Analyze the following {len(articles)} news articles about "{topic}" and extract the top trending subtopics.

TASK: Extract 5-8 specific trending subtopics as keywords from these articles.

RULES:
1. Focus on SPECIFIC trending themes, not general concepts
2. Extract keywords that represent current trends and hot topics
3. Avoid very general terms like "news" or "updates"
4. Prefer specific technologies, events, companies, or phenomena mentioned
5. Return ONLY the keywords, separated by commas
6. Each keyword should be 1-3 words maximum
7. Focus on what's currently trending or newsworthy
8. Don't be very specific, if you see that a theme will only appear once in one article, don't include it. The themes should be elligibe for a news feed.

ARTICLES TO ANALYZE:
{articles_text}

OUTPUT FORMAT: Return only the trending subtopic keywords separated by commas, nothing else.
Example: "AI regulation, ChatGPT updates, tech layoffs, startup funding, cybersecurity threats"
"""

        # Step 4: Get LLM analysis
        client = get_openai_client()
        if not client:
            return {
                "success": False,
                "error": "OpenAI client not available",
                "subtopics": []
            }
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a news analysis expert that extracts trending subtopics from news articles."},
                {"role": "user", "content": analysis_prompt}
            ],
            max_tokens=200,
            temperature=0.3  # Lower temperature for more consistent results
        )
        
        # Step 5: Parse LLM response
        llm_response = response.choices[0].message.content.strip()
        logger.info(f"LLM response: {llm_response}")
        
        # Extract subtopics from response
        subtopics = []
        if llm_response:
            # Split by commas and clean up
            raw_subtopics = [s.strip() for s in llm_response.split(',')]
            subtopics = [s for s in raw_subtopics if s and len(s) > 2]  # Filter out empty or too short
        
        logger.info(f"Extracted {len(subtopics)} trending subtopics: {subtopics}")
        
        return {
            "success": True,
            "topic": topic,
            "articles_analyzed": len(articles),
            "subtopics": subtopics,
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
        }
        
    except Exception as e:
        logger.error(f"Error extracting trending subtopics: {e}")
        return {
            "success": False,
            "error": str(e),
            "subtopics": []
        }

@https_fn.on_request(timeout_sec=120)
def get_trending_for_subtopic(req: https_fn.Request) -> https_fn.Response:
    """
    HTTP endpoint to get trending topics for a specific subtopic.
    
    Expected request body:
    {
        "subtopic_title": "Artificial Intelligence",
        "subtopic_query": "artificial intelligence OR AI",
        "subreddits": ["MachineLearning", "Artificial", "singularity"],
        "lang": "en",
        "country": "us",
        "max_articles": 10
    }
    """
    
    # Handle CORS preflight requests
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type, Authorization',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers, status=204)
    
    if req.method != 'POST':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response('Method not allowed. Use POST.', headers=headers, status=405)
    
    try:
        request_data = req.get_json()
        if not request_data:
            headers = {
                'Access-Control-Allow-Origin': '*',
                'Content-Type': 'application/json'
            }
            return https_fn.Response(
                json.dumps({"success": False, "error": "No JSON data provided"}),
                headers=headers,
                status=400
            )
        
        # Extract parameters
        subtopic_title = request_data.get('subtopic_title')
        subtopic_query = request_data.get('subtopic_query')
        subreddits = request_data.get('subreddits', [])
        
        if not subtopic_title or not subtopic_query:
            headers = {
                'Access-Control-Allow-Origin': '*',
                'Content-Type': 'application/json'
            }
            return https_fn.Response(
                json.dumps({"success": False, "error": "subtopic_title and subtopic_query are required"}),
                headers=headers,
                status=400
            )
        
        lang = request_data.get('lang', 'en')
        country = request_data.get('country', 'us')
        max_articles = request_data.get('max_articles', 10)
        
        logger.info(f"Getting trending topics for subtopic: {subtopic_title}")
        
        # Call the analysis function
        result = get_trending_topics_for_subtopic(
            subtopic_title=subtopic_title,
            subtopic_query=subtopic_query,
            subreddits=subreddits,
            lang=lang,
            country=country,
            max_articles=max_articles
        )
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        
        return https_fn.Response(
            json.dumps(result),
            headers=headers,
            status=200 if result.get("success") else 500
        )
        
    except Exception as e:
        logger.error(f"Error in get_trending_for_subtopic endpoint: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response(
            json.dumps({"success": False, "error": str(e), "trending_topics": []}),
            headers=headers,
            status=500
        )

@https_fn.on_request(timeout_sec=120)
def get_trending_subtopics(req: https_fn.Request) -> https_fn.Response:
    """
    HTTP endpoint to get trending subtopics for a given topic.
    
    Expected request body:
    {
        "topic": "technology",
        "lang": "en",
        "country": "us",
        "max_articles": 10
    }
    
    Returns:
    {
        "success": true,
        "topic": "technology",
        "articles_analyzed": 10,
        "subtopics": ["AI regulation", "ChatGPT updates", "tech layoffs", ...]
    }
    """
    
    # Handle CORS preflight requests
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type, Authorization',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers, status=204)
    
    if req.method != 'POST':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response('Method not allowed. Use POST.', headers=headers, status=405)
    
    try:
        # Parse request body
        request_data = req.get_json()
        if not request_data:
            headers = {
                'Access-Control-Allow-Origin': '*',
                'Content-Type': 'application/json'
            }
            return https_fn.Response(
                json.dumps({"success": False, "error": "No JSON data provided"}),
                headers=headers,
                status=400
            )
        
        # Extract parameters
        topic = request_data.get('topic')
        if not topic:
            headers = {
                'Access-Control-Allow-Origin': '*',
                'Content-Type': 'application/json'
            }
            return https_fn.Response(
                json.dumps({"success": False, "error": "Topic is required"}),
                headers=headers,
                status=400
            )
        
        lang = request_data.get('lang', 'en')
        country = request_data.get('country', 'us')
        max_articles = request_data.get('max_articles', 10)
        
        # Validate max_articles
        max_articles = min(max(1, max_articles), 20)  # Between 1 and 20
        
        logger.info(f"Getting trending subtopics for topic: {topic}, lang: {lang}, country: {country}")
        
        # Call the analysis function
        result = extract_trending_subtopics(
            topic=topic,
            lang=lang,
            country=country,
            max_articles=max_articles
        )
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        
        return https_fn.Response(
            json.dumps(result),
            headers=headers,
            status=200 if result.get("success") else 500
        )
        
    except Exception as e:
        logger.error(f"Error in get_trending_subtopics endpoint: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response(
            json.dumps({"success": False, "error": str(e), "subtopics": []}),
            headers=headers,
            status=500
        )

@https_fn.on_request(timeout_sec=30)
def get_user_preferences(req: https_fn.Request) -> https_fn.Response:
    """
    HTTP function to get user preferences for updating.
    
    Expected request:
    {
        "user_id": "user123"
    }
    
    Returns:
    {
        "success": true,
        "preferences": {
            "topics": ["world", "business"],
            "subtopics": {
                "AI": {"subreddits": [...], "queries": [...]},
                "Finance": {"subreddits": [...], "queries": [...]}
            },
            "detail_level": "Medium",
            "language": "en",
            "format_version": "2.0"
        }
    }
    """
    # Handle CORS preflight
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers)
    
    if req.method != 'POST':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response(
            json.dumps({"success": False, "error": "Method not allowed. Use POST."}),
            headers=headers,
            status=405
        )
    
    try:
        # Parse request data
        data = req.get_json()
        if not data:
            raise ValueError("No JSON data provided")
        
        user_id = data.get('user_id')
        if not user_id:
            raise ValueError("Missing user_id")
        
        logger.info(f"Getting preferences for user: {user_id}")
        
        # Get preferences from database
        preferences = get_user_preferences_from_db(user_id)
        
        if preferences:
            # Remove internal fields that shouldn't be sent to client
            if preferences.get('format_version') == '3.0':
                # New nested format
                client_preferences = {
                    'preferences': preferences.get('preferences', {}),
                    'detail_level': preferences.get('detail_level', 'Medium'),
                    'language': preferences.get('language', 'en'),
                    'format_version': '3.0'
                }
                
                topics_count = len(client_preferences['preferences'])
                subtopics_count = sum(len(topic_subtopics) for topic_subtopics in client_preferences['preferences'].values())
                
                logger.info(f"Successfully retrieved v3.0 preferences for user {user_id}")
                logger.info(f"  - Topics: {topics_count} items")
                logger.info(f"  - Subtopics: {subtopics_count} items")
                
            else:
                # Legacy format (v2.0 or older) - convert for backward compatibility
                client_preferences = {
                    'topics': preferences.get('topics', []),
                    'subtopics': preferences.get('subtopics', {}),
                    'detail_level': preferences.get('detail_level', 'Medium'),
                    'language': preferences.get('language', 'en'),
                    'format_version': preferences.get('format_version', '2.0'),
                    'specific_subjects': preferences.get('specific_subjects', [])  # Include for backward compatibility
                }
                
                logger.info(f"Successfully retrieved legacy preferences for user {user_id}")
                logger.info(f"  - Topics: {len(client_preferences['topics'])} items")
                logger.info(f"  - Subtopics: {len(client_preferences['subtopics'])} items")
            
            response_data = {
                "success": True,
                "preferences": client_preferences,
                "message": "Preferences retrieved successfully",
                "timestamp": datetime.now().isoformat()
            }
            
        else:
            # No preferences found - return empty structure in new format
            response_data = {
                "success": True,
                "preferences": {
                    'preferences': {},
                    'detail_level': 'Medium',
                    'language': 'en',
                    'format_version': '3.0'
                },
                "message": "No existing preferences found",
                "timestamp": datetime.now().isoformat()
            }
            
            logger.info(f"No preferences found for user {user_id}, returning empty v3.0 structure")
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        
        return https_fn.Response(json.dumps(response_data), headers=headers)
        
    except Exception as e:
        logger.error(f"Error in get_user_preferences: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "success": False,
            "error": str(e),
            "message": "An error occurred while retrieving preferences",
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)

def get_articles_subtopics_user(subtopic_name, subtopic_data, lang="en", country="us", include_comments=False, max_comments=3):
    """
    Fetch articles and Reddit posts for a user's subtopic.
    
    Args:
        subtopic_name (str): Name of the subtopic (e.g., "Finance")
        subtopic_data (dict): Subtopic data with format:
            {
                "subreddits": ["personalfinance", "stocks", "cryptocurrency"],
                "queries": ["stock market", "bitcoin", "interest rates"]
            }
        lang (str): Language code for GNews API
        country (str): Country code for GNews API
        include_comments (bool): Whether to include top comments for Reddit posts
        max_comments (int): Maximum number of top comments to fetch per post
    
    Returns:
        dict: Response with format:
            {
                "Finance": [top2_articles],
                "subreddits": {
                    "personalfinance": [
                        {
                            "title": "Post title",
                            "score": 123,
                            "url": "https://reddit.com/...",
                            "subreddit": "personalfinance",
                            "created_utc": 1716883200.0,
                            "num_comments": 45,
                            "author": "username",
                            "selftext": "Full post text...",
                            "comments": [  # Only if include_comments=True
                                {
                                    "body": "Comment text",
                                    "author": "commenter",
                                    "score": 67,
                                    "created_utc": 1716883300.0,
                                    "replies_count": 3,
                                    "is_submitter": false
                                }
                            ]
                        }
                    ]
                },
                "queries": {
                    "stock market": [top2_articles],
                    "bitcoin": [top2_articles], 
                    "interest rates": [top2_articles]
                }
            }
    """
    try:
        logger.info(f"Fetching articles and posts for subtopic: {subtopic_name}")
        
        result = {
            subtopic_name: [],
            "subreddits": {},
            "queries": {}
        }
        
        # Step 1: Fetch top 2 articles for the subtopic name itself
        logger.info(f"Fetching articles for subtopic name: {subtopic_name}")
        subtopic_response = gnews_search(
            query=subtopic_name,
            lang=lang,
            country=country,
            max_articles=2,
            from_date=(datetime.now() - timedelta(hours=24)).strftime("%Y-%m-%dT%H:%M:%SZ")
        )
        
        # Check if quota is exceeded from the first request
        quota_exceeded = False
        if not subtopic_response.get("success"):
            error_msg = subtopic_response.get("error", "")
            if "quota" in error_msg.lower() or "forbidden" in error_msg.lower():
                quota_exceeded = True
                logger.warning(f"GNews quota exceeded for subtopic '{subtopic_name}'. Queries will return empty results.")
        
        if subtopic_response.get("success") and subtopic_response.get("articles"):
            result[subtopic_name] = format_gnews_articles_for_prysm(subtopic_response)[:2]
            logger.info(f"Found {len(result[subtopic_name])} articles for subtopic name")
        else:
            logger.warning(f"No articles found for subtopic name: {subtopic_name}")
        
        # Add delay to avoid rate limiting
        time.sleep(1)
        
        # Step 2: Fetch top 2 articles for each query
        queries = subtopic_data.get("queries", [])
        logger.info(f"üîç SUBTOPIC DEBUG: Fetching articles for {len(queries)} queries: {queries}")
        
        for i, query in enumerate(queries):
            logger.info(f"üîç SUBTOPIC DEBUG: Processing query {i+1}/{len(queries)}: '{query}'")
            
            # Skip if quota already exceeded
            if quota_exceeded:
                result["queries"][query] = []
                logger.warning(f"‚ö†Ô∏è SUBTOPIC DEBUG: Skipping query '{query}' due to quota limit")
                continue
            
            # Add delay between requests to avoid rate limiting
            if i > 0:
                time.sleep(1)
            
            query_response = gnews_search(
                query=query,
                lang=lang,
                country=country,
                max_articles=2,
                from_date=(datetime.now() - timedelta(hours=24)).strftime("%Y-%m-%dT%H:%M:%SZ")
            )
            
            logger.info(f"üìä SUBTOPIC DEBUG: Query '{query}' response success: {query_response.get('success', False)}")
            logger.info(f"üìä SUBTOPIC DEBUG: Query '{query}' articles count: {len(query_response.get('articles', []))}")
            
            if query_response.get("success") and query_response.get("articles"):
                result["queries"][query] = format_gnews_articles_for_prysm(query_response)[:2]
                logger.info(f"‚úÖ SUBTOPIC DEBUG: Found {len(result['queries'][query])} articles for query: {query}")
            else:
                result["queries"][query] = []
                error_msg = query_response.get("error", "")
                
                logger.warning(f"‚ö†Ô∏è SUBTOPIC DEBUG: No articles for query '{query}'. Error: {error_msg}")
                
                if "quota" in error_msg.lower() or "forbidden" in error_msg.lower():
                    quota_exceeded = True
                    logger.warning(f"üö´ SUBTOPIC DEBUG: Daily quota exceeded for query: {query}. Skipping remaining queries.")
                    # Fill remaining queries with empty arrays
                    for remaining_query in queries[i+1:]:
                        result["queries"][remaining_query] = []
                        logger.warning(f"‚ö†Ô∏è SUBTOPIC DEBUG: Skipped remaining query: {remaining_query}")
                    break
                elif "rate limit" in error_msg.lower() or "too many" in error_msg.lower():
                    logger.warning(f"üö´ SUBTOPIC DEBUG: Rate limit hit for query: {query}. Skipping remaining queries.")
                    # Fill remaining queries with empty arrays
                    for remaining_query in queries[i+1:]:
                        result["queries"][remaining_query] = []
                        logger.warning(f"‚ö†Ô∏è SUBTOPIC DEBUG: Skipped remaining query: {remaining_query}")
                    break
                else:
                    logger.warning(f"‚ö†Ô∏è SUBTOPIC DEBUG: No articles found for query: {query} (not a quota/rate limit issue)")
        
        # Log quota status
        if quota_exceeded:
            logger.warning("GNews daily quota has been exceeded. Some queries returned no results.")
        
        # Step 3: Fetch top 2 posts from each subreddit
        subreddits = subtopic_data.get("subreddits", [])
        logger.info(f"Fetching posts from {len(subreddits)} subreddits")
        
        headers = {"User-Agent": "NewsXTrendingBot/1.0"}
        
        for subreddit in subreddits:
            logger.info(f"Fetching posts from r/{subreddit}")
            try:
                # Use 'top' endpoint with time filter for last 24 hours
                url = f"https://www.reddit.com/r/{subreddit}/top.json?t=day&limit=2"
                response = requests.get(url, headers=headers, timeout=10)
                response.raise_for_status()
                data = response.json()
                
                posts = []
                if "data" in data and "children" in data["data"]:
                    for post in data["data"]["children"]:
                        post_data = post.get("data", {})
                        
                        # Check if post is from last 24 hours
                        created_utc = post_data.get("created_utc", 0)
                        post_time = datetime.fromtimestamp(created_utc)
                        time_diff = datetime.now() - post_time
                        
                        if time_diff.total_seconds() <= 86400:  # 24 hours
                            posts.append({
                                "title": post_data.get("title", ""),
                                "score": post_data.get("score", 0),
                                "url": f"https://reddit.com{post_data.get('permalink', '')}",
                                "subreddit": subreddit,
                                "created_utc": created_utc,
                                "num_comments": post_data.get("num_comments", 0),
                                "author": post_data.get("author", ""),
                                "selftext": post_data.get("selftext", "")  # Return full selftext
                            })
                
                result["subreddits"][subreddit] = posts[:2]  # Top 2 posts
                logger.info(f"Found {len(result['subreddits'][subreddit])} posts from r/{subreddit}")
                
            except Exception as e:
                result["subreddits"][subreddit] = []
                logger.warning(f"Failed to fetch from r/{subreddit}: {e}")
        
        # Step 4: Fetch top comments for Reddit posts if requested
        if include_comments:
            logger.info(f"Fetching top {max_comments} comments for each Reddit post")
            for subreddit, posts in result["subreddits"].items():
                for post in posts:
                    # Extract permalink from the full URL
                    permalink = post["url"].replace("https://reddit.com", "")
                    comments = get_reddit_post_comments(permalink, max_comments)
                    post["comments"] = comments
                    logger.info(f"Added {len(comments)} comments to post: {post['title'][:50]}...")
        
        # Log summary
        total_articles = len(result[subtopic_name]) + sum(len(articles) for articles in result["queries"].values())
        total_posts = sum(len(posts) for posts in result["subreddits"].values())
        
        logger.info(f"Successfully fetched content for {subtopic_name}:")
        logger.info(f"  - Subtopic articles: {len(result[subtopic_name])}")
        logger.info(f"  - Query articles: {total_articles - len(result[subtopic_name])}")
        logger.info(f"  - Reddit posts: {total_posts}")
        
        return {
            "success": True,
            "data": result,
            "summary": {
                "subtopic_articles": len(result[subtopic_name]),
                "query_articles": sum(len(articles) for articles in result["queries"].values()),
                "reddit_posts": total_posts,
                "total_queries": len(queries),
                "total_subreddits": len(subreddits)
            },
            "warnings": {
                "quota_exceeded": quota_exceeded,
                "message": "GNews daily quota exceeded - some queries returned no results" if quota_exceeded else None
            }
        }
        
    except Exception as e:
        logger.error(f"Error in get_articles_subtopics_user: {e}")
        return {
            "success": False,
            "error": str(e),
            "data": {
                subtopic_name: [],
                "subreddits": {},
                "queries": {}
            }
        }

@https_fn.on_request(timeout_sec=120)
def get_articles_subtopics_user_endpoint(req: https_fn.Request) -> https_fn.Response:
    """
    HTTP endpoint to fetch articles and Reddit posts for a user's subtopic.
    
    Expected request (POST):
    {
        "subtopic_name": "Finance",
        "subtopic_data": {
            "subreddits": ["personalfinance", "stocks", "cryptocurrency"],
            "queries": ["stock market", "bitcoin", "interest rates"]
        },
        "lang": "en",
        "country": "us",
        "include_comments": false,  // Optional: whether to fetch top comments for Reddit posts
        "max_comments": 3          // Optional: max number of comments per post (default: 3)
    }
    
    Returns:
    {
        "success": true,
        "data": {
            "Finance": [top2_articles],
            "subreddits": {
                "personalfinance": [
                    {
                        "title": "Post title",
                        "score": 123,
                        "url": "https://reddit.com/...",
                        "subreddit": "personalfinance",
                        "created_utc": 1716883200.0,
                        "num_comments": 45,
                        "author": "username",
                        "selftext": "Full post text...",
                        "comments": [  // Only if include_comments=true
                            {
                                "body": "Comment text",
                                "author": "commenter",
                                "score": 67,
                                "created_utc": 1716883300.0,
                                "replies_count": 3,
                                "is_submitter": false,
                                "distinguished": null,
                                "stickied": false
                            }
                        ]
                    }
                ]
            },
            "queries": {
                "stock market": [top2_articles],
                "bitcoin": [top2_articles],
                "interest rates": [top2_articles]
            }
        },
        "summary": {
            "subtopic_articles": 2,
            "query_articles": 6,
            "reddit_posts": 6,
            "total_queries": 3,
            "total_subreddits": 3
        }
    }
    """
    # Handle CORS preflight
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers)
    
    if req.method != 'POST':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response(
            json.dumps({"success": False, "error": "Method not allowed. Use POST."}),
            headers=headers,
            status=405
        )
    
    try:
        # Parse request data
        data = req.get_json()
        if not data:
            raise ValueError("No JSON data provided")
        
        subtopic_name = data.get('subtopic_name')
        subtopic_data = data.get('subtopic_data')
        lang = data.get('lang', 'en')
        country = data.get('country', 'us')
        include_comments = data.get('include_comments', False)
        max_comments = data.get('max_comments', 3)
        
        # Validate required parameters
        if not subtopic_name:
            raise ValueError("Missing subtopic_name")
        
        if not subtopic_data:
            raise ValueError("Missing subtopic_data")
        
        if not isinstance(subtopic_data, dict):
            raise ValueError("subtopic_data must be an object")
        
        if 'subreddits' not in subtopic_data or 'queries' not in subtopic_data:
            raise ValueError("subtopic_data must contain 'subreddits' and 'queries' fields")
        
        if not isinstance(subtopic_data['subreddits'], list) or not isinstance(subtopic_data['queries'], list):
            raise ValueError("subreddits and queries must be arrays")
        
        logger.info(f"Fetching content for subtopic: {subtopic_name}")
        logger.info(f"  - Subreddits: {subtopic_data['subreddits']}")
        logger.info(f"  - Queries: {subtopic_data['queries']}")
        logger.info(f"  - Language: {lang}, Country: {country}")
        
        # Call the main function
        result = get_articles_subtopics_user(
            subtopic_name=subtopic_name,
            subtopic_data=subtopic_data,
            lang=lang,
            country=country,
            include_comments=include_comments,
            max_comments=max_comments
        )
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        
        return https_fn.Response(
            json.dumps(result),
            headers=headers,
            status=200 if result.get("success") else 500
        )
        
    except Exception as e:
        logger.error(f"Error in get_articles_subtopics_user_endpoint: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "success": False,
            "error": str(e),
            "message": "An error occurred while fetching subtopic content",
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)

def get_topic_posts(topic_name, topic_data, lang="en", country="us"):
    """
    Fetch articles and posts for a complete user topic with all its subtopics.
    
    Args:
        topic_name (str): Name of the topic (e.g., "business", "technology")
        topic_data (dict): Complete topic data from user preferences with format:
            {
                "Finance": {
                    "subreddits": ["personalfinance", "stocks", "cryptocurrency"],
                    "queries": ["stock market", "bitcoin", "interest rates"]
                },
                "Economy": {
                    "subreddits": ["economics", "investing"],
                    "queries": ["inflation", "GDP", "economic policy"]
                }
            }
        lang (str): Language code for GNews API
        country (str): Country code for GNews API
    
    Returns:
        dict: Response with format:
            {
                "topic_headlines": [top2_headlines_for_topic],
                "subtopics": {
                    "Finance": {
                        "Finance": [top2_articles],
                        "subreddits": {...},
                        "queries": {...}
                    },
                    "Economy": {
                        "Economy": [top2_articles],
                        "subreddits": {...},
                        "queries": {...}
                    }
                }
            }
    """
    try:
        logger.info(f"Fetching posts for topic: {topic_name} with {len(topic_data)} subtopics")
        
        result = {
            "topic_headlines": [],
            "subtopics": {}
        }
        
        # Step 1: Get top 2 headlines for the main topic
        logger.info(f"Fetching headlines for topic: {topic_name}")
        headlines_response = gnews_top_headlines(
            category=topic_name.lower(),
            lang=lang,
            country=country,
            max_articles=2,
            from_date=(datetime.now() - timedelta(hours=24)).strftime("%Y-%m-%dT%H:%M:%SZ")
        )
        
        if headlines_response.get("success") and headlines_response.get("articles"):
            result["topic_headlines"] = format_gnews_articles_for_prysm(headlines_response)[:2]
            logger.info(f"Found {len(result['topic_headlines'])} headlines for topic: {topic_name}")
        else:
            logger.warning(f"No headlines found for topic: {topic_name}")
            error_msg = headlines_response.get("error", "")
            if "quota" in error_msg.lower() or "forbidden" in error_msg.lower():
                logger.warning(f"GNews quota exceeded for topic headlines: {topic_name}")
        
        # Add delay to avoid rate limiting
        time.sleep(1)
        
        # Step 2: Process each subtopic using get_articles_subtopics_user
        subtopic_names = list(topic_data.keys())
        logger.info(f"Processing {len(subtopic_names)} subtopics: {subtopic_names}")
        
        for i, (subtopic_name, subtopic_data) in enumerate(topic_data.items()):
            logger.info(f"Processing subtopic {i+1}/{len(subtopic_names)}: {subtopic_name}")
            
            # Add delay between subtopics to avoid overwhelming APIs
            if i > 0:
                time.sleep(2)
            
            # Call the existing function for this subtopic
            subtopic_result = get_articles_subtopics_user(
                subtopic_name=subtopic_name,
                subtopic_data=subtopic_data,
                lang=lang,
                country=country
            )
            
            if subtopic_result.get("success"):
                result["subtopics"][subtopic_name] = subtopic_result.get("data", {})
                summary = subtopic_result.get("summary", {})
                logger.info(f"Subtopic '{subtopic_name}' completed: {summary.get('subtopic_articles', 0)} articles, {summary.get('query_articles', 0)} query articles, {summary.get('reddit_posts', 0)} posts")
            else:
                result["subtopics"][subtopic_name] = {
                    subtopic_name: [],
                    "subreddits": {},
                    "queries": {}
                }
                logger.error(f"Failed to process subtopic '{subtopic_name}': {subtopic_result.get('error', 'Unknown error')}")
        
        # Calculate summary statistics
        total_subtopic_articles = sum(
            len(subtopic_data.get(subtopic_name, [])) 
            for subtopic_name, subtopic_data in result["subtopics"].items()
        )
        total_query_articles = sum(
            sum(len(articles) for articles in subtopic_data.get("queries", {}).values())
            for subtopic_data in result["subtopics"].values()
        )
        total_reddit_posts = sum(
            sum(len(posts) for posts in subtopic_data.get("subreddits", {}).values())
            for subtopic_data in result["subtopics"].values()
        )
        
        logger.info(f"Topic '{topic_name}' processing completed:")
        logger.info(f"  - Topic headlines: {len(result['topic_headlines'])}")
        logger.info(f"  - Subtopics processed: {len(result['subtopics'])}")
        logger.info(f"  - Total subtopic articles: {total_subtopic_articles}")
        logger.info(f"  - Total query articles: {total_query_articles}")
        logger.info(f"  - Total Reddit posts: {total_reddit_posts}")
        
        return {
            "success": True,
            "data": result,
            "summary": {
                "topic_headlines": len(result["topic_headlines"]),
                "subtopics_processed": len(result["subtopics"]),
                "total_subtopic_articles": total_subtopic_articles,
                "total_query_articles": total_query_articles,
                "total_reddit_posts": total_reddit_posts
            }
        }
        
    except Exception as e:
        logger.error(f"Error in get_topic_posts: {e}")
        return {
            "success": False,
            "error": str(e),
            "data": {
                "topic_headlines": [],
                "subtopics": {}
            }
        }

@https_fn.on_request(timeout_sec=180)
def get_topic_posts_endpoint(req: https_fn.Request) -> https_fn.Response:
    """
    HTTP endpoint to fetch articles and Reddit posts for a complete user topic.
    
    Expected request (POST):
    {
        "topic_name": "business",
        "topic_data": {
            "Finance": {
                "subreddits": ["personalfinance", "stocks", "cryptocurrency"],
                "queries": ["stock market", "bitcoin", "interest rates"]
            },
            "Economy": {
                "subreddits": ["economics", "investing"],
                "queries": ["inflation", "GDP", "economic policy"]
            }
        },
        "lang": "en",
        "country": "us"
    }
    
    Returns:
    {
        "success": true,
        "data": {
            "topic_headlines": [top2_headlines_for_topic],
            "subtopics": {
                "Finance": {
                    "Finance": [top2_articles],
                    "subreddits": {...},
                    "queries": {...}
                },
                "Economy": {
                    "Economy": [top2_articles],
                    "subreddits": {...},
                    "queries": {...}
                }
            }
        },
        "summary": {
            "topic_headlines": 2,
            "subtopics_processed": 2,
            "total_subtopic_articles": 4,
            "total_query_articles": 12,
            "total_reddit_posts": 12
        }
    }
    """
    # Handle CORS preflight
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers)
    
    if req.method != 'POST':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response(
            json.dumps({"success": False, "error": "Method not allowed. Use POST."}),
            headers=headers,
            status=405
        )
    
    try:
        # Parse request data
        data = req.get_json()
        if not data:
            raise ValueError("No JSON data provided")
        
        topic_name = data.get('topic_name')
        topic_data = data.get('topic_data')
        lang = data.get('lang', 'en')
        country = data.get('country', 'us')
        
        # Validate required parameters
        if not topic_name:
            raise ValueError("Missing topic_name")
        
        if not topic_data:
            raise ValueError("Missing topic_data")
        
        if not isinstance(topic_data, dict):
            raise ValueError("topic_data must be an object")
        
        # Validate topic_data structure
        for subtopic_name, subtopic_data in topic_data.items():
            if not isinstance(subtopic_data, dict):
                raise ValueError(f"Subtopic '{subtopic_name}' must be an object")
            
            if 'subreddits' not in subtopic_data or 'queries' not in subtopic_data:
                raise ValueError(f"Subtopic '{subtopic_name}' must have 'subreddits' and 'queries' fields")
            
            if not isinstance(subtopic_data['subreddits'], list) or not isinstance(subtopic_data['queries'], list):
                raise ValueError(f"Subtopic '{subtopic_name}' subreddits and queries must be arrays")
        
        logger.info(f"Processing topic: {topic_name} with {len(topic_data)} subtopics")
        logger.info(f"  - Subtopics: {list(topic_data.keys())}")
        logger.info(f"  - Language: {lang}, Country: {country}")
        
        # Call the main function
        result = get_topic_posts(
            topic_name=topic_name,
            topic_data=topic_data,
            lang=lang,
            country=country
        )
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        
        return https_fn.Response(
            json.dumps(result),
            headers=headers,
            status=200 if result.get("success") else 500
        )
        
    except Exception as e:
        logger.error(f"Error in get_topic_posts_endpoint: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "success": False,
            "error": str(e),
            "message": "An error occurred while fetching topic content",
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)

def get_reddit_post_comments(post_permalink, max_comments=3):
    """
    Fetch top comments for a specific Reddit post.
    
    Args:
        post_permalink (str): Reddit post permalink (e.g., "/r/personalfinance/comments/abc123/post_title/")
        max_comments (int): Maximum number of top-level comments to fetch
    
    Returns:
        list: List of top comments with format:
            [
                {
                    "body": "Comment text",
                    "author": "username",
                    "score": 123,
                    "created_utc": 1716883200.0,
                    "replies_count": 5,
                    "is_submitter": false
                }
            ]
    """
    try:
        headers = {"User-Agent": "NewsXTrendingBot/1.0"}
        # Reddit comments API endpoint
        url = f"https://www.reddit.com{post_permalink}.json?limit={max_comments}&sort=top"
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        comments = []
        
        # Reddit returns an array with 2 elements: [post_data, comments_data]
        if len(data) >= 2 and "data" in data[1] and "children" in data[1]["data"]:
            comments_data = data[1]["data"]["children"]
            
            for comment in comments_data[:max_comments]:
                if comment.get("kind") == "t1":  # t1 = comment type
                    comment_data = comment.get("data", {})
                    
                    # Skip deleted/removed comments
                    if comment_data.get("body") in ["[deleted]", "[removed]"]:
                        continue
                    
                    # Count replies
                    replies_count = 0
                    if "replies" in comment_data and comment_data["replies"]:
                        if isinstance(comment_data["replies"], dict):
                            replies_data = comment_data["replies"].get("data", {})
                            if "children" in replies_data:
                                replies_count = len([r for r in replies_data["children"] if r.get("kind") == "t1"])
                    
                    comments.append({
                        "body": comment_data.get("body", ""),
                        "author": comment_data.get("author", ""),
                        "score": comment_data.get("score", 0),
                        "created_utc": comment_data.get("created_utc", 0),
                        "replies_count": replies_count,
                        "is_submitter": comment_data.get("is_submitter", False),
                        "distinguished": comment_data.get("distinguished"),  # mod/admin comments
                        "stickied": comment_data.get("stickied", False)
                    })
        
        logger.info(f"Fetched {len(comments)} comments for post {post_permalink}")
        return comments
        
    except Exception as e:
        logger.warning(f"Failed to fetch comments for {post_permalink}: {e}")
        return []

def get_articles_subtopics_user_with_comments(subtopic_name, subtopic_data, lang="en", country="us", include_comments=False, max_comments=3):
    """
    Fetch articles and Reddit posts for a user's subtopic, including top comments for Reddit posts.
    
    Args:
        subtopic_name (str): Name of the subtopic (e.g., "Finance")
        subtopic_data (dict): Subtopic data with format:
            {
                "subreddits": ["personalfinance", "stocks", "cryptocurrency"],
                "queries": ["stock market", "bitcoin", "interest rates"]
            }
        lang (str): Language code for GNews API
        country (str): Country code for GNews API
        include_comments (bool): Whether to include top comments for Reddit posts
        max_comments (int): Maximum number of top comments to fetch
    
    Returns:
        dict: Response with format:
            {
                "Finance": [top2_articles],
                "subreddits": {
                    "personalfinance": [top2_posts],
                    "stocks": [top2_posts],
                    "cryptocurrency": [top2_posts]
                },
                "queries": {
                    "stock market": [top2_articles],
                    "bitcoin": [top2_articles], 
                    "interest rates": [top2_articles]
                },
                "comments": [top_comments]
            }
    """
    try:
        logger.info(f"Fetching articles and posts for subtopic: {subtopic_name}")
        
        result = {
            subtopic_name: [],
            "subreddits": {},
            "queries": {},
            "comments": []
        }
        
        # Step 1: Fetch top 2 articles for the subtopic name itself
        logger.info(f"Fetching articles for subtopic name: {subtopic_name}")
        subtopic_response = gnews_search(
            query=subtopic_name,
            lang=lang,
            country=country,
            max_articles=2,
            from_date=(datetime.now() - timedelta(hours=24)).strftime("%Y-%m-%dT%H:%M:%SZ")
        )
        
        # Check if quota is exceeded from the first request
        quota_exceeded = False
        if not subtopic_response.get("success"):
            error_msg = subtopic_response.get("error", "")
            if "quota" in error_msg.lower() or "forbidden" in error_msg.lower():
                quota_exceeded = True
                logger.warning(f"GNews quota exceeded for subtopic '{subtopic_name}'. Queries will return empty results.")
        
        if subtopic_response.get("success") and subtopic_response.get("articles"):
            result[subtopic_name] = format_gnews_articles_for_prysm(subtopic_response)[:2]
            logger.info(f"Found {len(result[subtopic_name])} articles for subtopic name")
        else:
            logger.warning(f"No articles found for subtopic name: {subtopic_name}")
        
        # Add delay to avoid rate limiting
        time.sleep(1)
        
        # Step 2: Fetch top 2 articles for each query
        queries = subtopic_data.get("queries", [])
        logger.info(f"Fetching articles for {len(queries)} queries")
        
        for i, query in enumerate(queries):
            logger.info(f"Fetching articles for query: {query}")
            
            # Skip if quota already exceeded
            if quota_exceeded:
                result["queries"][query] = []
                logger.warning(f"Skipping query '{query}' due to quota limit")
                continue
            
            # Add delay between requests to avoid rate limiting
            if i > 0:
                time.sleep(1)
            
            query_response = gnews_search(
                query=query,
                lang=lang,
                country=country,
                max_articles=2,
                from_date=(datetime.now() - timedelta(hours=24)).strftime("%Y-%m-%dT%H:%M:%SZ")
            )
            
            if query_response.get("success") and query_response.get("articles"):
                result["queries"][query] = format_gnews_articles_for_prysm(query_response)[:2]
                logger.info(f"Found {len(result['queries'][query])} articles for query: {query}")
            else:
                result["queries"][query] = []
                error_msg = query_response.get("error", "")
                
                if "quota" in error_msg.lower() or "forbidden" in error_msg.lower():
                    quota_exceeded = True
                    logger.warning(f"Daily quota exceeded for query: {query}. Skipping remaining queries.")
                    # Fill remaining queries with empty arrays
                    for remaining_query in queries[i+1:]:
                        result["queries"][remaining_query] = []
                    break
                elif "rate limit" in error_msg.lower() or "too many" in error_msg.lower():
                    logger.warning(f"Rate limit hit for query: {query}. Skipping remaining queries.")
                    # Fill remaining queries with empty arrays
                    for remaining_query in queries[i+1:]:
                        result["queries"][remaining_query] = []
                    break
                else:
                    logger.warning(f"No articles found for query: {query}")
        
        # Log quota status
        if quota_exceeded:
            logger.warning("GNews daily quota has been exceeded. Some queries returned no results.")
        
        # Step 3: Fetch top 2 posts from each subreddit
        subreddits = subtopic_data.get("subreddits", [])
        logger.info(f"Fetching posts from {len(subreddits)} subreddits")
        
        headers = {"User-Agent": "NewsXTrendingBot/1.0"}
        
        for subreddit in subreddits:
            logger.info(f"Fetching posts from r/{subreddit}")
            try:
                # Use 'top' endpoint with time filter for last 24 hours
                url = f"https://www.reddit.com/r/{subreddit}/top.json?t=day&limit=2"
                response = requests.get(url, headers=headers, timeout=10)
                response.raise_for_status()
                data = response.json()
                
                posts = []
                if "data" in data and "children" in data["data"]:
                    for post in data["data"]["children"]:
                        post_data = post.get("data", {})
                        
                        # Check if post is from last 24 hours
                        created_utc = post_data.get("created_utc", 0)
                        post_time = datetime.fromtimestamp(created_utc)
                        time_diff = datetime.now() - post_time
                        
                        if time_diff.total_seconds() <= 86400:  # 24 hours
                            posts.append({
                                "title": post_data.get("title", ""),
                                "score": post_data.get("score", 0),
                                "url": f"https://reddit.com{post_data.get('permalink', '')}",
                                "subreddit": subreddit,
                                "created_utc": created_utc,
                                "num_comments": post_data.get("num_comments", 0),
                                "author": post_data.get("author", ""),
                                "selftext": post_data.get("selftext", "")  # Return full selftext
                            })
                
                result["subreddits"][subreddit] = posts[:2]  # Top 2 posts
                logger.info(f"Found {len(result['subreddits'][subreddit])} posts from r/{subreddit}")
                
            except Exception as e:
                result["subreddits"][subreddit] = []
                logger.warning(f"Failed to fetch from r/{subreddit}: {e}")
        
        # Step 4: Fetch top comments for Reddit posts if requested
        if include_comments:
            logger.info(f"Fetching top {max_comments} comments for each Reddit post")
            for subreddit, posts in result["subreddits"].items():
                for post in posts:
                    # Extract permalink from the full URL
                    permalink = post["url"].replace("https://reddit.com", "")
                    comments = get_reddit_post_comments(permalink, max_comments)
                    post["comments"] = comments
                    logger.info(f"Added {len(comments)} comments to post: {post['title'][:50]}...")
        
        # Log summary
        total_articles = len(result[subtopic_name]) + sum(len(articles) for articles in result["queries"].values())
        total_posts = sum(len(posts) for posts in result["subreddits"].values())
        
        logger.info(f"Successfully fetched content for {subtopic_name}:")
        logger.info(f"  - Subtopic articles: {len(result[subtopic_name])}")
        logger.info(f"  - Query articles: {total_articles - len(result[subtopic_name])}")
        logger.info(f"  - Reddit posts: {total_posts}")
        
        return {
            "success": True,
            "data": result,
            "summary": {
                "subtopic_articles": len(result[subtopic_name]),
                "query_articles": sum(len(articles) for articles in result["queries"].values()),
                "reddit_posts": total_posts,
                "total_queries": len(queries),
                "total_subreddits": len(subreddits)
            },
            "warnings": {
                "quota_exceeded": quota_exceeded,
                "message": "GNews daily quota exceeded - some queries returned no results" if quota_exceeded else None
            }
        }
        
    except Exception as e:
        logger.error(f"Error in get_articles_subtopics_user_with_comments: {e}")
        return {
            "success": False,
            "error": str(e),
            "data": {
                subtopic_name: [],
                "subreddits": {},
                "queries": {},
                "comments": []
            }
        }

def get_pickup_line(topic_name, topic_content_data):
    """
    Generate an engaging 1-sentence pickup line for a topic based on retrieved content.
    
    Args:
        topic_name (str): Name of the topic (e.g., "Business", "Technology")
        topic_content_data (dict): Complete topic data from get_topic_posts() with format:
            {
                "success": True,
                "data": {
                    "topic_headlines": [articles],
                    "subtopics": {
                        "Finance": {
                            "Finance": [articles],
                            "subreddits": {"personalfinance": [posts]},
                            "queries": {"stock market": [articles]}
                        }
                    }
                }
            }
    
    Returns:
        dict: Response with format:
            {
                "success": True,
                "pickup_line": "One engaging sentence to entice user to click...",
                "topic_name": "Business",
                "content_summary": {
                    "total_articles": 15,
                    "subtopics_count": 3,
                    "trending_keywords": ["AI", "stocks", "inflation"]
                }
            }
    """
    try:
        logger.info(f"Generating pickup line for topic: {topic_name}")
        
        if not topic_content_data.get("success"):
            raise ValueError(f"Invalid topic content data: {topic_content_data.get('error', 'Unknown error')}")
        
        data = topic_content_data.get("data", {})
        
        # Extract and summarize all content (focus on articles only, not Reddit)
        content_summary = {
            "total_articles": 0,
            "subtopics_count": 0,
            "trending_keywords": [],
            "key_headlines": []
        }
        
        # Count topic headlines
        topic_headlines = data.get("topic_headlines", [])
        content_summary["total_articles"] += len(topic_headlines)
        
        # Extract key headlines from topic
        for article in topic_headlines[:3]:  # Top 3 headlines
            if article.get("title"):
                content_summary["key_headlines"].append(article["title"])
        
        # Process subtopics
        subtopics = data.get("subtopics", {})
        content_summary["subtopics_count"] = len(subtopics)
        
        for subtopic_name, subtopic_data in subtopics.items():
            # Count subtopic articles
            subtopic_articles = subtopic_data.get(subtopic_name, [])
            content_summary["total_articles"] += len(subtopic_articles)
            
            # Extract headlines from subtopic articles
            for article in subtopic_articles[:2]:  # Top 2 per subtopic
                if article.get("title"):
                    content_summary["key_headlines"].append(article["title"])
            
            # Count query articles and add queries as trending keywords
            queries = subtopic_data.get("queries", {})
            for query, articles in queries.items():
                content_summary["total_articles"] += len(articles)
                content_summary["trending_keywords"].append(query)
        
        # Limit arrays to prevent overwhelming the LLM
        content_summary["key_headlines"] = content_summary["key_headlines"][:6]
        content_summary["trending_keywords"] = list(set(content_summary["trending_keywords"]))[:5]
        
        # Create prompt for OpenAI
        prompt = f"""You are a master content curator and copywriter. Your task is to create an irresistible 1-sentence pickup line that will make users want to click and explore the "{topic_name}" topic.

TOPIC CONTENT SUMMARY:
- Topic: {topic_name}
- Total Articles: {content_summary['total_articles']}
- Subtopics: {content_summary['subtopics_count']} ({', '.join(subtopics.keys()) if subtopics else 'None'})
- Trending Keywords: {', '.join(content_summary['trending_keywords']) if content_summary['trending_keywords'] else 'None'}

KEY HEADLINES:
{chr(10).join([f"‚Ä¢ {headline}" for headline in content_summary['key_headlines'][:4]])}

INSTRUCTIONS:
1. Write EXACTLY 1 sentence that creates curiosity and urgency
2. Use specific details from the headlines above
3. Make it feel like breaking news or insider information
4. Include numbers, trends, or surprising facts when possible
5. Focus ONLY on news articles and developments (no social media references)
6. Keep it conversational and engaging, not salesy
7. Focus on what's happening RIGHT NOW in this topic

EXAMPLE STYLE:
"üö® Breaking: [Specific trend/number] is shaking up [topic] as [specific detail from headlines] sends shockwaves through the industry."

Generate the pickup line now:"""

        # Get OpenAI client and generate response
        client = get_openai_client()
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an expert copywriter who creates irresistible content hooks. Always respond with exactly 1 sentence that creates maximum curiosity and engagement."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=100,
            temperature=0.8
        )
        
        pickup_line = response.choices[0].message.content.strip()
        
        logger.info(f"Generated pickup line for {topic_name}: {pickup_line[:100]}...")
        
        return {
            "success": True,
            "pickup_line": pickup_line,
            "topic_name": topic_name,
            "content_summary": content_summary,
            "generation_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error generating pickup line for {topic_name}: {e}")
        return {
            "success": False,
            "error": str(e),
            "topic_name": topic_name,
            "pickup_line": f"Discover what's trending in {topic_name} right now with breaking stories and latest developments.",
            "fallback": True
        }

@https_fn.on_request(timeout_sec=60)
def get_pickup_line_endpoint(req: https_fn.Request) -> https_fn.Response:
    """
    HTTP endpoint to generate pickup lines for topics.
    
    Expected request (POST):
    {
        "topic_name": "Business",
        "topic_content_data": {
            "success": true,
            "data": {
                "topic_headlines": [...],
                "subtopics": {...}
            }
        }
    }
    
    Returns:
    {
        "success": true,
        "pickup_line": "One engaging sentence...",
        "topic_name": "Business",
        "content_summary": {
            "total_articles": 15,
            "subtopics_count": 3,
            "trending_keywords": ["AI", "stocks", "inflation"]
        }
    }
    """
    # Handle CORS preflight
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers)
    
    if req.method != 'POST':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response(
            json.dumps({"success": False, "error": "Method not allowed. Use POST."}),
            headers=headers,
            status=405
        )
    
    try:
        # Parse request data
        data = req.get_json()
        if not data:
            raise ValueError("No JSON data provided")
        
        topic_name = data.get('topic_name')
        topic_content_data = data.get('topic_content_data')
        
        # Validate required parameters
        if not topic_name:
            raise ValueError("Missing topic_name")
        
        if not topic_content_data:
            raise ValueError("Missing topic_content_data")
        
        if not isinstance(topic_content_data, dict):
            raise ValueError("topic_content_data must be an object")
        
        logger.info(f"Generating pickup line for topic: {topic_name}")
        
        # Call the main function
        result = get_pickup_line(
            topic_name=topic_name,
            topic_content_data=topic_content_data
        )
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        
        return https_fn.Response(
            json.dumps(result),
            headers=headers,
            status=200 if result.get("success") else 500
        )
        
    except Exception as e:
        logger.error(f"Error in get_pickup_line_endpoint: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "success": False,
            "error": str(e),
            "message": "An error occurred while generating pickup line",
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)

def get_topic_summary(topic_name, topic_content_data):
    """
    Generate a comprehensive summary of all topic content with key facts from each article.
    
    Args:
        topic_name (str): Name of the topic (e.g., "Business", "Technology")
        topic_content_data (dict): Complete topic data from get_topic_posts() with format:
            {
                "success": True,
                "data": {
                    "topic_headlines": [articles],
                    "subtopics": {
                        "Finance": {
                            "Finance": [articles],
                            "subreddits": {"personalfinance": [posts]},
                            "queries": {"stock market": [articles]}
                        }
                    }
                }
            }
    
    Returns:
        dict: Response with format:
            {
                "success": True,
                "topic_summary": "Comprehensive formatted summary with key facts...",
                "topic_name": "Business",
                "content_stats": {
                    "total_articles": 15,
                    "total_posts": 8,
                    "subtopics_analyzed": 3
                }
            }
    """
    try:
        logger.info(f"Generating comprehensive summary for topic: {topic_name}")
        
        if not topic_content_data.get("success"):
            raise ValueError(f"Invalid topic content data: {topic_content_data.get('error', 'Unknown error')}")
        
        data = topic_content_data.get("data", {})
        
        # Collect all content for analysis
        all_content = {
            "topic_headlines": [],
            "subtopic_articles": {},
            "query_articles": {},
            "reddit_discussions": {}
        }
        
        content_stats = {
            "total_articles": 0,
            "total_posts": 0,
            "subtopics_analyzed": 0
        }
        
        # Collect topic headlines
        topic_headlines = data.get("topic_headlines", [])
        all_content["topic_headlines"] = topic_headlines
        content_stats["total_articles"] += len(topic_headlines)
        
        # Collect subtopic content
        subtopics = data.get("subtopics", {})
        content_stats["subtopics_analyzed"] = len(subtopics)
        
        for subtopic_name, subtopic_data in subtopics.items():
            # Collect subtopic articles
            subtopic_articles = subtopic_data.get(subtopic_name, [])
            if subtopic_articles:
                all_content["subtopic_articles"][subtopic_name] = subtopic_articles
                content_stats["total_articles"] += len(subtopic_articles)
            
            # Collect query articles
            queries = subtopic_data.get("queries", {})
            for query, articles in queries.items():
                if articles:
                    all_content["query_articles"][f"{subtopic_name} - {query}"] = articles
                    content_stats["total_articles"] += len(articles)
            
            # Collect Reddit discussions
            subreddits = subtopic_data.get("subreddits", {})
            for subreddit, posts in subreddits.items():
                if posts:
                    all_content["reddit_discussions"][f"{subtopic_name} - r/{subreddit}"] = posts
                    content_stats["total_posts"] += len(posts)
        
        # Prepare content for LLM analysis
        content_text = f"TOPIC: {topic_name}\n\n"
        
        # Add topic headlines
        if all_content["topic_headlines"]:
            content_text += "üî• MAIN TOPIC HEADLINES:\n"
            for i, article in enumerate(all_content["topic_headlines"], 1):
                title = article.get('title', 'No title')
                snippet = article.get('snippet', article.get('description', ''))
                source = article.get('source', 'Unknown source')
                content_text += f"{i}. {title}\n"
                content_text += f"   Source: {source}\n"
                if snippet:
                    content_text += f"   Summary: {snippet}\n"
                content_text += "\n"
        
        # Add subtopic articles
        if all_content["subtopic_articles"]:
            content_text += "üìä SUBTOPIC ARTICLES:\n"
            for subtopic_name, articles in all_content["subtopic_articles"].items():
                content_text += f"\n{subtopic_name.upper()}:\n"
                for i, article in enumerate(articles, 1):
                    title = article.get('title', 'No title')
                    snippet = article.get('snippet', article.get('description', ''))
                    source = article.get('source', 'Unknown source')
                    content_text += f"  {i}. {title}\n"
                    content_text += f"     Source: {source}\n"
                    if snippet:
                        content_text += f"     Summary: {snippet}\n"
                content_text += "\n"
        
        # Add query-based articles
        if all_content["query_articles"]:
            content_text += "üîç QUERY-BASED ARTICLES:\n"
            for query_label, articles in all_content["query_articles"].items():
                content_text += f"\n{query_label.upper()}:\n"
                for i, article in enumerate(articles, 1):
                    title = article.get('title', 'No title')
                    snippet = article.get('snippet', article.get('description', ''))
                    source = article.get('source', 'Unknown source')
                    content_text += f"  {i}. {title}\n"
                    content_text += f"     Source: {source}\n"
                    if snippet:
                        content_text += f"     Summary: {snippet}\n"
                content_text += "\n"
        
        # Add Reddit discussions
        if all_content["reddit_discussions"]:
            content_text += "üí¨ REDDIT DISCUSSIONS:\n"
            for subreddit_label, posts in all_content["reddit_discussions"].items():
                content_text += f"\n{subreddit_label.upper()}:\n"
                for i, post in enumerate(posts, 1):
                    title = post.get('title', 'No title')
                    score = post.get('score', 0)
                    selftext = post.get('selftext', '')
                    content_text += f"  {i}. {title} ({score} upvotes)\n"
                    if selftext and len(selftext) > 10:
                        content_text += f"     Content: {selftext[:200]}...\n"
                content_text += "\n"
        
        # Create LLM prompt for summary generation
        summary_prompt = f"""You are an expert news analyst and content summarizer. Create a comprehensive, well-formatted summary of the {topic_name} topic based on all the content below.

INSTRUCTIONS:
1. Create a structured summary with clear sections
2. Extract key facts from each article/post mentioned
3. Present information in a scannable, organized format
4. Use emojis and formatting to make it visually appealing
5. Group related information logically
6. Highlight the most important trends and developments
7. Keep each fact brief but informative (1-2 sentences max)
8. Include source attribution where relevant
9. Make it feel like a premium news briefing

FORMAT STRUCTURE:
üìà **{topic_name.upper()} BRIEFING**

üî• **TOP HEADLINES**
‚Ä¢ [Key fact from headline 1]
‚Ä¢ [Key fact from headline 2]

üìä **SUBTOPIC INSIGHTS**
**[Subtopic Name]**
‚Ä¢ [Key fact 1]
‚Ä¢ [Key fact 2]

üîç **TRENDING SEARCHES**
‚Ä¢ [Key developments from query-based articles]

üí¨ **COMMUNITY BUZZ**
‚Ä¢ [Notable Reddit discussions and insights]

‚ö° **KEY TAKEAWAYS**
‚Ä¢ [3-4 most important points from all content]

CONTENT TO ANALYZE:
{content_text}

Generate the comprehensive summary now:"""

        # Get OpenAI client and generate response
        client = get_openai_client()
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an expert news analyst who creates comprehensive, well-formatted topic summaries. Always use clear structure, emojis, and extract key facts from all provided content."},
                {"role": "user", "content": summary_prompt}
            ],
            max_tokens=1500,
            temperature=0.7
        )
        
        topic_summary = response.choices[0].message.content.strip()
        
        logger.info(f"Generated comprehensive summary for {topic_name}: {len(topic_summary)} characters")
        
        return {
            "success": True,
            "topic_summary": topic_summary,
            "topic_name": topic_name,
            "content_stats": content_stats,
            "generation_timestamp": datetime.now().isoformat(),
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
        }
        
    except Exception as e:
        logger.error(f"Error generating topic summary for {topic_name}: {e}")
        return {
            "success": False,
            "error": str(e),
            "topic_name": topic_name,
            "topic_summary": f"# {topic_name} Summary\n\nUnable to generate detailed summary at this time. Please try again later.",
            "fallback": True
        }

@https_fn.on_request(timeout_sec=90)
def get_topic_summary_endpoint(req: https_fn.Request) -> https_fn.Response:
    """
    HTTP endpoint to generate comprehensive topic summaries.
    
    Expected request (POST):
    {
        "topic_name": "Business",
        "topic_content_data": {
            "success": true,
            "data": {
                "topic_headlines": [...],
                "subtopics": {...}
            }
        }
    }
    
    Returns:
    {
        "success": true,
        "topic_summary": "Comprehensive formatted summary...",
        "topic_name": "Business",
        "content_stats": {
            "total_articles": 15,
            "total_posts": 8,
            "subtopics_analyzed": 3
        }
    }
    """
    # Handle CORS preflight
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers)
    
    if req.method != 'POST':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response(
            json.dumps({"success": False, "error": "Method not allowed. Use POST."}),
            headers=headers,
            status=405
        )
    
    try:
        # Parse request data
        data = req.get_json()
        if not data:
            raise ValueError("No JSON data provided")
        
        topic_name = data.get('topic_name')
        topic_content_data = data.get('topic_content_data')
        
        # Validate required parameters
        if not topic_name:
            raise ValueError("Missing topic_name")
        
        if not topic_content_data:
            raise ValueError("Missing topic_content_data")
        
        if not isinstance(topic_content_data, dict):
            raise ValueError("topic_content_data must be an object")
        
        logger.info(f"Generating comprehensive summary for topic: {topic_name}")
        
        # Call the main function
        result = get_topic_summary(
            topic_name=topic_name,
            topic_content_data=topic_content_data
        )
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        
        return https_fn.Response(
            json.dumps(result),
            headers=headers,
            status=200 if result.get("success") else 500
        )
        
    except Exception as e:
        logger.error(f"Error in get_topic_summary_endpoint: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "success": False,
            "error": str(e),
            "message": "An error occurred while generating topic summary",
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)

def get_reddit_world_summary(reddit_posts_data):
    """
    Generate a concise executive summary of Reddit posts and comments focused on world events.
    Filters out personal content and presents key trends/developments in a professional format.
    
    Args:
        reddit_posts_data (list): List of Reddit posts with comments in format:
            [
                {
                    "title": "Post title",
                    "score": 123,
                    "url": "https://reddit.com/...",
                    "subreddit": "worldnews",
                    "selftext": "Post content...",
                    "comments": [
                        {
                            "body": "Comment text",
                            "author": "username",
                            "score": 67
                        }
                    ]
                }
            ]
    
    Returns:
        dict: Response with format:
            {
                "success": True,
                "world_summary": "Executive summary of world events and trends...",
                "posts_analyzed": 15,
                "relevant_posts": 8,
                "key_topics": ["topic1", "topic2"]
            }
    """
    try:
        logger.info(f"Generating world summary from {len(reddit_posts_data)} Reddit posts")
        
        if not reddit_posts_data:
            return {
                "success": True,
                "world_summary": "No significant world events or trends detected in current Reddit discussions.",
                "posts_analyzed": 0,
                "relevant_posts": 0,
                "key_topics": []
            }
        
        # Filter and prepare content for analysis
        relevant_content = []
        personal_keywords = [
            "my", "i am", "i'm", "my wife", "my husband", "my job", "my boss", 
            "personal", "relationship", "dating", "family", "parents", "kids",
            "salary", "debt", "loan", "mortgage", "credit card", "budget"
        ]
        
        world_keywords = [
            "government", "politics", "economy", "market", "global", "international",
            "country", "nation", "war", "conflict", "trade", "policy", "election",
            "climate", "technology", "industry", "company", "stock", "inflation",
            "gdp", "unemployment", "regulation", "law", "court", "supreme court"
        ]
        
        relevant_posts = 0
        
        for post in reddit_posts_data:
            title = post.get('title', '').lower()
            selftext = post.get('selftext', '').lower()
            subreddit = post.get('subreddit', '').lower()
            score = post.get('score', 0)
            
            # Skip if too personal
            is_personal = any(keyword in title or keyword in selftext for keyword in personal_keywords)
            
            # Check if relevant to world events
            is_world_relevant = (
                any(keyword in title or keyword in selftext for keyword in world_keywords) or
                subreddit in ['worldnews', 'news', 'politics', 'economics', 'technology', 'business'] or
                score > 100  # High-scoring posts are often newsworthy
            )
            
            if is_world_relevant and not is_personal:
                relevant_posts += 1
                
                # Prepare post content
                post_content = f"SUBREDDIT: r/{post.get('subreddit', 'unknown')}\n"
                post_content += f"TITLE: {post.get('title', 'No title')} ({score} upvotes)\n"
                
                if selftext and len(selftext.strip()) > 20:
                    post_content += f"CONTENT: {selftext[:300]}...\n"
                
                # Add top comments (filter out personal ones)
                comments = post.get('comments', [])
                relevant_comments = []
                
                for comment in comments[:5]:  # Top 5 comments
                    comment_body = comment.get('body', '').lower()
                    comment_score = comment.get('score', 0)
                    
                    # Skip personal comments
                    if not any(keyword in comment_body for keyword in personal_keywords):
                        if comment_score > 10 or any(keyword in comment_body for keyword in world_keywords):
                            relevant_comments.append({
                                'body': comment.get('body', '')[:200],
                                'score': comment_score
                            })
                
                if relevant_comments:
                    post_content += "TOP COMMENTS:\n"
                    for i, comment in enumerate(relevant_comments[:3], 1):
                        post_content += f"  {i}. {comment['body']} ({comment['score']} upvotes)\n"
                
                relevant_content.append(post_content)
        
        if not relevant_content:
            return {
                "success": True,
                "world_summary": "No significant world events or trends detected in current Reddit discussions.",
                "posts_analyzed": len(reddit_posts_data),
                "relevant_posts": 0,
                "key_topics": []
            }
        
        # Create executive summary prompt
        content_text = "\n\n".join(relevant_content[:10])  # Limit to top 10 relevant posts
        
        summary_prompt = f"""You are an executive assistant briefing your boss on current world events and trends based on Reddit discussions. 

INSTRUCTIONS:
1. Focus ONLY on world events, politics, economy, technology, and business trends
2. Ignore personal stories, relationship advice, or individual financial situations
3. Present information as if briefing a busy executive (concise, professional)
4. Highlight what's trending, what people are discussing, and key developments
5. MAXIMUM 50 words total
6. Use bullet points with markdown formatting for easy scanning
7. Focus on actionable insights and trends

FORMAT:
**Key Developments:**
‚Ä¢ [Major trend/event 1]
‚Ä¢ [Major trend/event 2]

REDDIT DISCUSSIONS TO ANALYZE:
{content_text}

Generate the executive brief (MAX 50 words):"""

        # Get OpenAI client and generate response
        client = get_openai_client()
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an executive assistant who creates concise, professional briefs on world events from social media discussions. Focus on trends, not personal stories."},
                {"role": "user", "content": summary_prompt}
            ],
            max_tokens=300,
            temperature=0.6
        )
        
        world_summary = response.choices[0].message.content.strip()
        
        # Extract key topics mentioned
        key_topics = []
        topic_keywords = ["trump", "biden", "china", "russia", "ukraine", "ai", "crypto", "inflation", "recession", "election", "climate", "energy"]
        for keyword in topic_keywords:
            if keyword in content_text.lower():
                key_topics.append(keyword.title())
        
        logger.info(f"Generated world summary from {relevant_posts} relevant posts out of {len(reddit_posts_data)} total")
        
        return {
            "success": True,
            "world_summary": world_summary,
            "posts_analyzed": len(reddit_posts_data),
            "relevant_posts": relevant_posts,
            "key_topics": key_topics[:6],  # Top 6 topics
            "generation_timestamp": datetime.now().isoformat(),
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
        }
        
    except Exception as e:
        logger.error(f"Error generating Reddit world summary: {e}")
        return {
            "success": False,
            "error": str(e),
            "world_summary": "Unable to generate world summary at this time.",
            "posts_analyzed": len(reddit_posts_data) if reddit_posts_data else 0,
            "relevant_posts": 0,
            "key_topics": []
        }

@https_fn.on_request(timeout_sec=60)
def get_reddit_world_summary_endpoint(req: https_fn.Request) -> https_fn.Response:
    """
    HTTP endpoint to generate executive world summaries from Reddit posts.
    
    Expected request (POST):
    {
        "reddit_posts": [
            {
                "title": "Post title",
                "score": 123,
                "url": "https://reddit.com/...",
                "subreddit": "worldnews",
                "selftext": "Post content...",
                "comments": [
                    {
                        "body": "Comment text",
                        "author": "username",
                        "score": 67
                    }
                ]
            }
        ]
    }
    
    Returns:
    {
        "success": true,
        "world_summary": "Executive summary of world events...",
        "posts_analyzed": 15,
        "relevant_posts": 8,
        "key_topics": ["Trump", "AI", "China"]
    }
    """
    # Handle CORS preflight
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers)
    
    if req.method != 'POST':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response(
            json.dumps({"success": False, "error": "Method not allowed. Use POST."}),
            headers=headers,
            status=405
        )
    
    try:
        # Parse request data
        data = req.get_json()
        if not data:
            raise ValueError("No JSON data provided")
        
        reddit_posts = data.get('reddit_posts', [])
        
        # Validate required parameters
        if not isinstance(reddit_posts, list):
            raise ValueError("reddit_posts must be an array")
        
        logger.info(f"Generating world summary from {len(reddit_posts)} Reddit posts")
        
        # Call the main function
        result = get_reddit_world_summary(reddit_posts_data=reddit_posts)
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        
        return https_fn.Response(
            json.dumps(result),
            headers=headers,
            status=200 if result.get("success") else 500
        )
        
    except Exception as e:
        logger.error(f"Error in get_reddit_world_summary_endpoint: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "success": False,
            "error": str(e),
            "message": "An error occurred while generating Reddit world summary",
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)

def get_complete_topic_report(topic_name, topic_posts_data):
    """
    Generate a complete topic report with pickup line, topic summary, and subtopic breakdowns.
    
    Args:
        topic_name (str): Name of the topic (e.g., "Business", "Technology")
        topic_posts_data (dict): Complete output from get_topic_posts() with format:
            {
                "success": True,
                "data": {
                    "topic_headlines": [articles],
                    "subtopics": {
                        "Finance": {
                            "Finance": [articles],
                            "subreddits": {"personalfinance": [posts]},
                            "queries": {"stock market": [articles]}
                        }
                    }
                }
            }
    
    Returns:
        dict: Complete report with format:
            {
                "success": True,
                "pickup_line": "Engaging 3-sentence hook...",
                "topic_summary": "Comprehensive topic overview...",
                "subtopics": {
                    "Finance": {
                        "subtopic_summary": "Summary of Finance articles...",
                        "reddit_summary": "Executive brief of Finance Reddit discussions..."
                    }
                }
            }
    """
    try:
        logger.info(f"Generating complete topic report for: {topic_name}")
        
        if not topic_posts_data.get("success"):
            raise ValueError(f"Invalid topic posts data: {topic_posts_data.get('error', 'Unknown error')}")
        
        report = {
            "success": True,
            "topic_name": topic_name,
            "pickup_line": "",
            "topic_summary": "",
            "subtopics": {},
            "generation_stats": {
                "pickup_line_generated": False,
                "topic_summary_generated": False,
                "subtopics_processed": 0,
                "total_subtopics": 0
            }
        }
        
        data = topic_posts_data.get("data", {})
        subtopics_data = data.get("subtopics", {})
        report["generation_stats"]["total_subtopics"] = len(subtopics_data)
        
        # Step 1: Generate pickup line for the entire topic
        logger.info(f"Step 1: Generating pickup line for {topic_name}")
        try:
            pickup_result = get_pickup_line(topic_name, topic_posts_data)
            if pickup_result.get("success"):
                report["pickup_line"] = pickup_result["pickup_line"]
                report["generation_stats"]["pickup_line_generated"] = True
                logger.info("‚úÖ Pickup line generated successfully")
            else:
                report["pickup_line"] = f"Discover what's trending in {topic_name} right now with breaking stories and latest developments."
                logger.warning(f"Pickup line generation failed, using fallback")
        except Exception as e:
            logger.error(f"Error generating pickup line: {e}")
            report["pickup_line"] = f"Stay updated with the latest {topic_name} news and trends."
        
        # Step 2: Generate comprehensive topic summary
        logger.info(f"Step 2: Generating topic summary for {topic_name}")
        try:
            summary_result = get_topic_summary(topic_name, topic_posts_data)
            if summary_result.get("success"):
                report["topic_summary"] = summary_result["topic_summary"]
                report["generation_stats"]["topic_summary_generated"] = True
                logger.info("‚úÖ Topic summary generated successfully")
            else:
                report["topic_summary"] = f"# {topic_name} Summary\n\nComprehensive overview of current {topic_name} developments and trends."
                logger.warning(f"Topic summary generation failed, using fallback")
        except Exception as e:
            logger.error(f"Error generating topic summary: {e}")
            report["topic_summary"] = f"Current {topic_name} overview and key developments."
        
        # Step 3: Process each subtopic
        logger.info(f"Step 3: Processing {len(subtopics_data)} subtopics")
        
        for subtopic_name, subtopic_data in subtopics_data.items():
            logger.info(f"Processing subtopic: {subtopic_name}")
            
            subtopic_report = {
                "subtopic_summary": "",
                "reddit_summary": ""
            }
            
            # 3a: Generate subtopic summary (articles + query articles)
            try:
                # Collect all articles for this subtopic
                subtopic_articles = subtopic_data.get(subtopic_name, [])
                query_articles = []
                
                queries_data = subtopic_data.get("queries", {})
                for query, articles in queries_data.items():
                    query_articles.extend(articles)
                
                all_subtopic_articles = subtopic_articles + query_articles
                
                if all_subtopic_articles:
                    # Create a mini topic summary for just this subtopic's articles
                    subtopic_content = {
                        "success": True,
                        "data": {
                            "topic_headlines": all_subtopic_articles,
                            "subtopics": {}
                        }
                    }
                    
                    summary_result = get_topic_summary(subtopic_name, subtopic_content)
                    if summary_result.get("success"):
                        subtopic_report["subtopic_summary"] = summary_result["topic_summary"]
                        logger.info(f"‚úÖ Generated summary for {subtopic_name} ({len(all_subtopic_articles)} articles)")
                    else:
                        subtopic_report["subtopic_summary"] = f"**{subtopic_name} Overview**\n\nKey developments and trends in {subtopic_name}."
                        logger.warning(f"Subtopic summary failed for {subtopic_name}")
                else:
                    subtopic_report["subtopic_summary"] = f"**{subtopic_name}**\n\nNo recent articles available for this subtopic."
                    logger.info(f"No articles found for {subtopic_name}")
                    
            except Exception as e:
                logger.error(f"Error generating subtopic summary for {subtopic_name}: {e}")
                subtopic_report["subtopic_summary"] = f"**{subtopic_name}**\n\nSummary unavailable."
            
            # 3b: Generate Reddit world summary for this subtopic
            try:
                # Collect all Reddit posts for this subtopic
                all_reddit_posts = []
                subreddits_data = subtopic_data.get("subreddits", {})
                
                for subreddit, posts in subreddits_data.items():
                    all_reddit_posts.extend(posts)
                
                if all_reddit_posts:
                    reddit_result = get_reddit_world_summary(all_reddit_posts)
                    if reddit_result.get("success"):
                        subtopic_report["reddit_summary"] = reddit_result["world_summary"]
                        logger.info(f"‚úÖ Generated Reddit summary for {subtopic_name} ({len(all_reddit_posts)} posts, {reddit_result.get('relevant_posts', 0)} relevant)")
                    else:
                        subtopic_report["reddit_summary"] = f"**{subtopic_name} Community Pulse**\n\nNo significant world events detected in current discussions."
                        logger.warning(f"Reddit summary failed for {subtopic_name}")
                else:
                    subtopic_report["reddit_summary"] = f"**{subtopic_name} Community Pulse**\n\nNo recent Reddit discussions available."
                    logger.info(f"No Reddit posts found for {subtopic_name}")
                    
            except Exception as e:
                logger.error(f"Error generating Reddit summary for {subtopic_name}: {e}")
                subtopic_report["reddit_summary"] = f"**{subtopic_name} Community Pulse**\n\nCommunity insights unavailable."
            
            report["subtopics"][subtopic_name] = subtopic_report
            report["generation_stats"]["subtopics_processed"] += 1
            
            logger.info(f"‚úÖ Completed processing {subtopic_name}")
        
        # Final statistics
        logger.info(f"Complete topic report generated for {topic_name}:")
        logger.info(f"  - Pickup line: {'‚úÖ' if report['generation_stats']['pickup_line_generated'] else '‚ùå'}")
        logger.info(f"  - Topic summary: {'‚úÖ' if report['generation_stats']['topic_summary_generated'] else '‚ùå'}")
        logger.info(f"  - Subtopics processed: {report['generation_stats']['subtopics_processed']}/{report['generation_stats']['total_subtopics']}")
        
        report["generation_timestamp"] = datetime.now().isoformat()
        
        return report
        
    except Exception as e:
        logger.error(f"Error generating complete topic report for {topic_name}: {e}")
        return {
            "success": False,
            "error": str(e),
            "topic_name": topic_name,
            "pickup_line": f"Explore the latest {topic_name} developments.",
            "topic_summary": f"# {topic_name}\n\nReport generation failed. Please try again.",
            "subtopics": {}
        }

@https_fn.on_request(timeout_sec=300)
def get_complete_topic_report_endpoint(req: https_fn.Request) -> https_fn.Response:
    """
    HTTP endpoint to generate complete topic reports.
    
    Expected request (POST):
    {
        "topic_name": "Business",
        "topic_posts_data": {
            "success": true,
            "data": {
                "topic_headlines": [...],
                "subtopics": {...}
            }
        }
    }
    
    Returns:
    {
        "success": true,
        "topic_name": "Business",
        "pickup_line": "Engaging 3-sentence hook...",
        "topic_summary": "Comprehensive topic overview...",
        "subtopics": {
            "Finance": {
                "subtopic_summary": "Summary of Finance articles...",
                "reddit_summary": "Executive brief of Finance Reddit discussions..."
            }
        },
        "generation_stats": {
            "pickup_line_generated": true,
            "topic_summary_generated": true,
            "subtopics_processed": 2,
            "total_subtopics": 2
        }
    }
    """
    # Handle CORS preflight
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers)
    
    if req.method != 'POST':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response(
            json.dumps({"success": False, "error": "Method not allowed. Use POST."}),
            headers=headers,
            status=405
        )
    
    try:
        # Parse request data
        data = req.get_json()
        if not data:
            raise ValueError("No JSON data provided")
        
        topic_name = data.get('topic_name')
        topic_posts_data = data.get('topic_posts_data')
        
        # Validate required parameters
        if not topic_name:
            raise ValueError("Missing topic_name")
        
        if not topic_posts_data:
            raise ValueError("Missing topic_posts_data")
        
        if not isinstance(topic_posts_data, dict):
            raise ValueError("topic_posts_data must be an object")
        
        logger.info(f"Generating complete topic report for: {topic_name}")
        
        # Call the main function
        result = get_complete_topic_report(
            topic_name=topic_name,
            topic_posts_data=topic_posts_data
        )
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        
        return https_fn.Response(
            json.dumps(result),
            headers=headers,
            status=200 if result.get("success") else 500
        )
        
    except Exception as e:
        logger.error(f"Error in get_complete_topic_report_endpoint: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "success": False,
            "error": str(e),
            "message": "An error occurred while generating complete topic report",
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)

def refresh_articles(user_id):
    """
    Refresh all articles for a user by fetching content for each topic in their preferences
    and storing the results in the database.
    
    Args:
        user_id (str): User ID to refresh articles for
    
    Returns:
        dict: Response with format:
            {
                "success": True,
                "user_id": "user123",
                "topics_processed": 3,
                "total_articles": 45,
                "total_posts": 28,
                "refresh_timestamp": "2025-05-28T21:30:00Z",
                "topics": {
                    "business": {
                        "success": True,
                        "articles_count": 15,
                        "posts_count": 10
                    }
                }
            }
    """
    try:
        logger.info(f"Starting article refresh for user: {user_id}")
        
        # Step 1: Get user preferences
        user_preferences = get_user_preferences_from_db(user_id)
        
        if not user_preferences:
            logger.warning(f"No preferences found for user {user_id}")
            return {
                "success": False,
                "error": "No user preferences found",
                "user_id": user_id,
                "topics_processed": 0
            }
        
        # Check if preferences are in new nested format (v3.0)
        format_version = user_preferences.get('format_version', '2.0')
        
        if format_version != '3.0' or 'preferences' not in user_preferences:
            logger.error(f"User {user_id} preferences are not in v3.0 nested format")
            return {
                "success": False,
                "error": "User preferences must be in v3.0 nested format",
                "user_id": user_id,
                "topics_processed": 0
            }
        
        nested_preferences = user_preferences['preferences']
        lang = user_preferences.get('language', 'en')
        country = 'us' if lang == 'en' else 'fr' if lang == 'fr' else 'us'
        
        logger.info(f"Found {len(nested_preferences)} topics for user {user_id}")
        
        # Step 2: Process each topic
        refresh_result = {
            "success": True,
            "user_id": user_id,
            "topics_processed": 0,
            "total_articles": 0,
            "total_posts": 0,
            "refresh_timestamp": datetime.now().isoformat(),
            "topics": {}
        }
        
        all_topics_data = {}
        
        for topic_name, topic_subtopics in nested_preferences.items():
            logger.info(f"Processing topic: {topic_name} with {len(topic_subtopics)} subtopics")
            
            try:
                # Call get_topic_posts for this topic
                topic_result = get_topic_posts(
                    topic_name=topic_name,
                    topic_data=topic_subtopics,
                    lang=lang,
                    country=country
                )
                
                if topic_result.get("success"):
                    # Count articles and posts
                    data = topic_result.get("data", {})
                    topic_headlines = len(data.get("topic_headlines", []))
                    
                    topic_articles = topic_headlines
                    topic_posts = 0
                    
                    subtopics = data.get("subtopics", {})
                    for subtopic_name, subtopic_data in subtopics.items():
                        topic_articles += len(subtopic_data.get(subtopic_name, []))
                        topic_articles += sum(len(articles) for articles in subtopic_data.get("queries", {}).values())
                        topic_posts += sum(len(posts) for posts in subtopic_data.get("subreddits", {}).values())
                    
                    refresh_result["topics"][topic_name] = {
                        "success": True,
                        "articles_count": topic_articles,
                        "posts_count": topic_posts,
                        "subtopics_count": len(subtopics)
                    }
                    
                    refresh_result["total_articles"] += topic_articles
                    refresh_result["total_posts"] += topic_posts
                    refresh_result["topics_processed"] += 1
                    
                    # Store the complete topic data
                    all_topics_data[topic_name] = topic_result
                    
                    logger.info(f"‚úÖ Topic {topic_name}: {topic_articles} articles, {topic_posts} posts")
                    
                else:
                    refresh_result["topics"][topic_name] = {
                        "success": False,
                        "error": topic_result.get("error", "Unknown error"),
                        "articles_count": 0,
                        "posts_count": 0
                    }
                    logger.error(f"‚ùå Failed to fetch topic {topic_name}: {topic_result.get('error', 'Unknown error')}")
                
                # Add delay between topics to avoid overwhelming APIs
                time.sleep(2)
                
            except Exception as e:
                refresh_result["topics"][topic_name] = {
                    "success": False,
                    "error": str(e),
                    "articles_count": 0,
                    "posts_count": 0
                }
                logger.error(f"‚ùå Error processing topic {topic_name}: {e}")
        
        # Step 3: Store in database
        if all_topics_data:
            try:
                db_client = firestore.client()
                
                # Prepare document for articles collection
                articles_document = {
                    "user_id": user_id,
                    "refresh_timestamp": refresh_result["refresh_timestamp"],
                    "topics_data": all_topics_data,
                    "summary": {
                        "topics_processed": refresh_result["topics_processed"],
                        "total_articles": refresh_result["total_articles"],
                        "total_posts": refresh_result["total_posts"],
                        "language": lang,
                        "country": country
                    },
                    "format_version": "1.0"
                }
                
                # Store in articles collection with user_id as document ID
                doc_ref = db_client.collection('articles').document(user_id)
                doc_ref.set(articles_document)
                
                logger.info(f"‚úÖ Stored articles for user {user_id} in database")
                refresh_result["database_stored"] = True
                
            except Exception as e:
                logger.error(f"‚ùå Failed to store articles in database: {e}")
                refresh_result["database_stored"] = False
                refresh_result["database_error"] = str(e)
        else:
            logger.warning(f"No topics data to store for user {user_id}")
            refresh_result["database_stored"] = False
            refresh_result["database_error"] = "No topics data available"
        
        # Final summary
        logger.info(f"Article refresh completed for user {user_id}:")
        logger.info(f"  - Topics processed: {refresh_result['topics_processed']}")
        logger.info(f"  - Total articles: {refresh_result['total_articles']}")
        logger.info(f"  - Total posts: {refresh_result['total_posts']}")
        logger.info(f"  - Database stored: {refresh_result.get('database_stored', False)}")
        
        return refresh_result
        
    except Exception as e:
        logger.error(f"Error refreshing articles for user {user_id}: {e}")
        return {
            "success": False,
            "error": str(e),
            "user_id": user_id,
            "topics_processed": 0,
            "total_articles": 0,
            "total_posts": 0
        }

@https_fn.on_request(timeout_sec=600)
def refresh_articles_endpoint(req: https_fn.Request) -> https_fn.Response:
    """
    HTTP endpoint to refresh articles for a user.
    
    Expected request (POST):
    {
        "user_id": "user123"
    }
    
    Returns:
    {
        "success": true,
        "user_id": "user123",
        "topics_processed": 3,
        "total_articles": 45,
        "total_posts": 28,
        "refresh_timestamp": "2025-05-28T21:30:00Z",
        "database_stored": true,
        "topics": {
            "business": {
                "success": true,
                "articles_count": 15,
                "posts_count": 10,
                "subtopics_count": 2
            },
            "technology": {
                "success": true,
                "articles_count": 20,
                "posts_count": 12,
                "subtopics_count": 3
            }
        }
    }
    """
    # Handle CORS preflight
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers)
    
    if req.method != 'POST':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response(
            json.dumps({"success": False, "error": "Method not allowed. Use POST."}),
            headers=headers,
            status=405
        )
    
    try:
        # Parse request data
        data = req.get_json()
        if not data:
            raise ValueError("No JSON data provided")
        
        user_id = data.get('user_id')
        
        # Validate required parameters
        if not user_id:
            raise ValueError("Missing user_id")
        
        logger.info(f"Starting article refresh for user: {user_id}")
        
        # Call the main function
        result = refresh_articles(user_id=user_id)
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        
        return https_fn.Response(
            json.dumps(result),
            headers=headers,
            status=200 if result.get("success") else 500
        )
        
    except Exception as e:
        logger.error(f"Error in refresh_articles_endpoint: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "success": False,
            "error": str(e),
            "message": "An error occurred while refreshing articles",
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)

def get_user_articles_from_db(user_id):
    """
    Get stored articles for a user from the database.
    
    Args:
        user_id (str): User ID
    
    Returns:
        dict: Stored articles data or None if not found
    """
    try:
        db_client = firestore.client()
        doc_ref = db_client.collection('articles').document(user_id)
        doc = doc_ref.get()
        
        if doc.exists:
            data = doc.to_dict()
            logger.info(f"Retrieved stored articles for user {user_id}")
            return data
        else:
            logger.info(f"No stored articles found for user {user_id}")
            return None
            
    except Exception as e:
        logger.error(f"Error retrieving articles for user {user_id}: {e}")
        return None

@https_fn.on_request(timeout_sec=30)
def get_user_articles_endpoint(req: https_fn.Request) -> https_fn.Response:
    """
    HTTP endpoint to get stored articles for a user.
    
    Expected request (POST):
    {
        "user_id": "user123"
    }
    
    Returns stored articles data or 404 if not found.
    """
    # Handle CORS preflight
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers)
    
    if req.method != 'POST':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response(
            json.dumps({"success": False, "error": "Method not allowed. Use POST."}),
            headers=headers,
            status=405
        )
    
    try:
        # Parse request data
        data = req.get_json()
        if not data:
            raise ValueError("No JSON data provided")
        
        user_id = data.get('user_id')
        
        if not user_id:
            raise ValueError("Missing user_id")
        
        # Get articles from database
        articles_data = get_user_articles_from_db(user_id)
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        
        if articles_data:
            return https_fn.Response(
                json.dumps({
                    "success": True,
                    "found": True,
                    "data": articles_data
                }),
                headers=headers
            )
        else:
            return https_fn.Response(
                json.dumps({
                    "success": True,
                    "found": False,
                    "message": "No articles found for this user"
                }),
                headers=headers,
                status=404
            )
        
    except Exception as e:
        logger.error(f"Error in get_user_articles_endpoint: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "success": False,
            "error": str(e),
            "message": "An error occurred while retrieving articles",
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)

def get_complete_report(user_id):
    """
    Generate complete reports for all topics for a user.
    Gets articles from database and calls get_complete_topic_report for each topic.
    
    Args:
        user_id (str): User ID to get articles for
    
    Returns:
        dict: Complete reports for all topics with format:
            {
                "success": True,
                "user_id": "user123",
                "reports": {
                    "business": {
                        "pickup_line": "...",
                        "topic_summary": "...",
                        "subtopics": {...}
                    },
                    "technology": {...}
                },
                "generation_stats": {
                    "topics_processed": 5,
                    "total_topics": 9,
                    "successful_reports": 5,
                    "failed_reports": 0
                }
            }
    """
    try:
        logger.info(f"Generating complete report for user: {user_id}")
        
        # Step 1: Get user articles from database
        articles_data = get_user_articles_from_db(user_id)
        
        if not articles_data:
            return {
                "success": False,
                "error": "No articles found for user",
                "user_id": user_id,
                "reports": {},
                "generation_stats": {
                    "topics_processed": 0,
                    "total_topics": 0,
                    "successful_reports": 0,
                    "failed_reports": 0
                }
            }
        
        topics = articles_data.get("topics_data", {})
        
        if not topics:
            return {
                "success": False,
                "error": "No topics found for user",
                "user_id": user_id,
                "reports": {},
                "generation_stats": {
                    "topics_processed": 0,
                    "total_topics": 0,
                    "successful_reports": 0,
                    "failed_reports": 0
                }
            }
        
        logger.info(f"Found {len(topics)} topics for user {user_id}")
        
        # Step 2: Initialize response
        complete_report = {
            "success": True,
            "user_id": user_id,
            "reports": {},
            "generation_stats": {
                "topics_processed": 0,
                "total_topics": len(topics),
                "successful_reports": 0,
                "failed_reports": 0
            },
            "refresh_timestamp": articles_data.get("refresh_timestamp"),
            "language": articles_data.get("summary", {}).get("language", "en")
        }
        
        # Step 3: Process each topic
        for topic_name, topic_data in topics.items():
            logger.info(f"Processing topic: {topic_name}")
            complete_report["generation_stats"]["topics_processed"] += 1
            
            try:
                # Call get_complete_topic_report for this topic
                topic_report = get_complete_topic_report(topic_name, topic_data)
                
                if topic_report.get("success"):
                    complete_report["reports"][topic_name] = {
                        "pickup_line": topic_report.get("pickup_line", ""),
                        "topic_summary": topic_report.get("topic_summary", ""),
                        "subtopics": topic_report.get("subtopics", {}),
                        "generation_stats": topic_report.get("generation_stats", {})
                    }
                    complete_report["generation_stats"]["successful_reports"] += 1
                    logger.info(f"‚úÖ Successfully generated report for {topic_name}")
                else:
                    # Store failed report with fallback content
                    complete_report["reports"][topic_name] = {
                        "pickup_line": f"Discover the latest {topic_name} developments and trends.",
                        "topic_summary": f"# {topic_name}\n\nReport generation failed. Please try again.",
                        "subtopics": {},
                        "generation_stats": {"error": topic_report.get("error", "Unknown error")}
                    }
                    complete_report["generation_stats"]["failed_reports"] += 1
                    logger.warning(f"‚ùå Failed to generate report for {topic_name}: {topic_report.get('error')}")
                    
            except Exception as e:
                logger.error(f"Error processing topic {topic_name}: {e}")
                complete_report["reports"][topic_name] = {
                    "pickup_line": f"Explore the latest {topic_name} news and insights.",
                    "topic_summary": f"# {topic_name}\n\nReport generation encountered an error.",
                    "subtopics": {},
                    "generation_stats": {"error": str(e)}
                }
                complete_report["generation_stats"]["failed_reports"] += 1
        
        # Step 4: Final statistics
        logger.info(f"Complete report generation finished for user {user_id}:")
        logger.info(f"  - Topics processed: {complete_report['generation_stats']['topics_processed']}")
        logger.info(f"  - Successful reports: {complete_report['generation_stats']['successful_reports']}")
        logger.info(f"  - Failed reports: {complete_report['generation_stats']['failed_reports']}")
        
        complete_report["generation_timestamp"] = datetime.now().isoformat()
        
        # Step 5: Save to aifeed collection

        
        try:
            logger.info(f"Saving complete report to aifeed collection for user {user_id}")
            db_client = firestore.client()
            aifeed_ref = db_client.collection('aifeed').document(user_id)
            
            # Prepare data for storage
            aifeed_data = {
                "user_id": user_id,
                "reports": complete_report["reports"],
                "generation_stats": complete_report["generation_stats"],
                "generation_timestamp": complete_report["generation_timestamp"],
                "refresh_timestamp": complete_report["refresh_timestamp"],
                "language": complete_report["language"],
                "format_version": "1.0"
            }
            
            # Save to database
            aifeed_ref.set(aifeed_data)
            logger.info(f"‚úÖ Complete report saved to aifeed collection for user {user_id}")
            
            # Add database storage confirmation to response
            complete_report["database_stored"] = True
            complete_report["aifeed_collection"] = "aifeed"
            
        except Exception as e:
            logger.error(f"Error saving complete report to aifeed collection for user {user_id}: {e}")
            complete_report["database_stored"] = False
            complete_report["database_error"] = str(e)
        
        return complete_report
        
    except Exception as e:
        logger.error(f"Error generating complete report for user {user_id}: {e}")
        return {
            "success": False,
            "error": str(e),
            "user_id": user_id,
            "reports": {},
            "generation_stats": {
                "topics_processed": 0,
                "total_topics": 0,
                "successful_reports": 0,
                "failed_reports": 0
            }
        }

@https_fn.on_request(timeout_sec=600)
def get_complete_report_endpoint(req: https_fn.Request) -> https_fn.Response:
    """
    HTTP endpoint to generate complete reports for all user topics.
    
    Expected request (POST):
    {
        "user_id": "6YV8wgIEBrev7e2Ep7fm0InByq02"
    }
    
    Returns:
    {
        "success": true,
        "user_id": "user123",
        "reports": {
            "business": {
                "pickup_line": "...",
                "topic_summary": "...",
                "subtopics": {...}
            }
        },
        "generation_stats": {...}
    }
    """
    # Enable CORS
    if req.method == "OPTIONS":
        headers = {
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "POST",
            "Access-Control-Allow-Headers": "Content-Type",
            "Access-Control-Max-Age": "3600",
        }
        return https_fn.Response("", status=204, headers=headers)

    headers = {
        "Access-Control-Allow-Origin": "*",
        "Content-Type": "application/json",
    }

    try:
        # Parse request
        request_json = req.get_json(silent=True)
        if not request_json:
            return https_fn.Response(
                json.dumps({"success": False, "error": "No JSON data provided"}),
                status=400,
                headers=headers
            )

        user_id = request_json.get("user_id")
        if not user_id:
            return https_fn.Response(
                json.dumps({"success": False, "error": "user_id is required"}),
                status=400,
                headers=headers
            )

        logger.info(f"Complete report request for user: {user_id}")

        # Generate complete report
        result = get_complete_report(user_id)

        if result.get("success"):
            logger.info(f"Complete report generated successfully for user {user_id}")
            return https_fn.Response(
                json.dumps(result),
                status=200,
                headers=headers
            )
        else:
            logger.error(f"Complete report generation failed for user {user_id}: {result.get('error')}")
            return https_fn.Response(
                json.dumps(result),
                status=500,
                headers=headers
            )

    except Exception as e:
        logger.error(f"Error in get_complete_report_endpoint: {e}")
        return https_fn.Response(
            json.dumps({"success": False, "error": str(e)}),
            status=500,
            headers=headers
        )

def get_aifeed_reports(user_id):
    """
    Get AI feed reports for a user from the aifeed collection.
    
    Args:
        user_id (str): User ID
    
    Returns:
        dict: AI feed reports data or None if not found
    """
    try:
        db_client = firestore.client()
        doc_ref = db_client.collection('aifeed').document(user_id)
        doc = doc_ref.get()
        
        if doc.exists:
            data = doc.to_dict()
            logger.info(f"Retrieved AI feed reports for user {user_id}")
            return {
                "success": True,
                "found": True,
                "data": data
            }
        else:
            logger.info(f"No AI feed reports found for user {user_id}")
            return {
                "success": True,
                "found": False,
                "message": "No AI feed reports found for this user"
            }
            
    except Exception as e:
        logger.error(f"Error retrieving AI feed reports for user {user_id}: {e}")
        return {
            "success": False,
            "error": str(e)
        }

@https_fn.on_request(timeout_sec=30)
def get_aifeed_reports_endpoint(req: https_fn.Request) -> https_fn.Response:
    """
    HTTP endpoint to get AI feed reports for a user.
    
    Expected request (POST):
    {
        "user_id": "user123"
    }
    
    Returns AI feed reports data or 404 if not found.
    """
    # Handle CORS preflight
    if req.method == 'OPTIONS':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type',
            'Access-Control-Max-Age': '3600'
        }
        return https_fn.Response('', headers=headers)
    
    if req.method != 'POST':
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        return https_fn.Response(
            json.dumps({"success": False, "error": "Method not allowed. Use POST."}),
            headers=headers,
            status=405
        )
    
    try:
        # Parse request data
        data = req.get_json()
        if not data:
            raise ValueError("No JSON data provided")
        
        user_id = data.get('user_id')
        
        if not user_id:
            raise ValueError("Missing user_id")
        
        # Get AI feed reports from database
        result = get_aifeed_reports(user_id)
        
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        
        if result.get("success"):
            if result.get("found"):
                return https_fn.Response(
                    json.dumps(result),
                    headers=headers
                )
            else:
                return https_fn.Response(
                    json.dumps(result),
                    headers=headers,
                    status=404
                )
        else:
            return https_fn.Response(
                json.dumps(result),
                headers=headers,
                status=500
            )
        
    except Exception as e:
        logger.error(f"Error in get_aifeed_reports_endpoint: {e}")
        headers = {
            'Access-Control-Allow-Origin': '*',
            'Content-Type': 'application/json'
        }
        error_response = {
            "success": False,
            "error": str(e),
            "message": "An error occurred while retrieving AI feed reports",
            "timestamp": datetime.now().isoformat()
        }
        return https_fn.Response(json.dumps(error_response), headers=headers, status=500)